{
  "version": "1.0",
  "project_root": "modular-framework/modules/llm-workflows",
  "dry_run": false,
  "backup": true,
  "changes": [
    {
      "id": "replace-logger-with-gateway-style",
      "description": "Replace llm-workflows logger with llm-gateway style dynamic logger (in-memory buffer, HEC sink, request stamping, hot-reloadable config). Service name set to 'llm-workflows'.",
      "op": "write_file",
      "path": "server/logger.js",
      "mode": "overwrite",
      "content": "// modular-framework/modules/llm-workflows/server/logger.js\nconst os = require('os');\nconst path = require('path');\n\n// ===== Defaults from ENV (backward-compatible) =====\nconst ENV_DEFAULTS = {\n  level: (process.env.LOG_LEVEL || 'info').toLowerCase(),\n  console: (process.env.LOG_TO_CONSOLE || 'false').toLowerCase() === 'true',\n  buffer_max: Number(process.env.LOG_MAX || 1000),\n  sinks: {\n    hec: {\n      enabled: !!(process.env.SPLUNK_HEC_URL && process.env.SPLUNK_HEC_TOKEN),\n      url: process.env.SPLUNK_HEC_URL || null,\n      token: process.env.SPLUNK_HEC_TOKEN || null,\n      source: process.env.SPLUNK_SOURCE || 'llm-workflows',\n      index: process.env.SPLUNK_INDEX || undefined,\n      tls_verify: String(process.env.NODE_TLS_REJECT_UNAUTHORIZED || '1') !== '0',\n      timeout_ms: 3000,\n      batch_max: 100\n    }\n  },\n  fields: { service: 'llm-workflows', host: os.hostname() },\n  sampling: { rate: 1.0 },\n  level_overrides: {}\n};\n\n// ===== In-memory log buffer =====\nconst logs = [];\nfunction pushBuffer(entry, cfg) {\n  logs.push(entry);\n  const max = Math.max(1, Number(cfg.buffer_max || 1000));\n  while (logs.length > max) logs.shift();\n}\n\n// ===== Utilities =====\nlet reqCounter = 0;\nfunction stamp(req, _res, next) {\n  req.id = `${Date.now().toString(36)}-${(++reqCounter).toString(36)}`;\n  next();\n}\nfunction redact(obj) {\n  if (!obj || typeof obj !== 'object') return obj;\n  const clone = JSON.parse(JSON.stringify(obj));\n  const hide = (o, k) => { if (o && o[k]) o[k] = '***REDACTED***'; };\n  hide(clone, 'apiKey'); hide(clone, 'token'); hide(clone, 'Authorization'); hide(clone, 'authorization');\n  if (clone.headers && clone.headers.Authorization) clone.headers.Authorization = '***REDACTED***';\n  if (clone.headers && clone.headers.authorization) clone.headers.authorization = '***REDACTED***';\n  if (Array.isArray(clone.redact)) clone.redact = ['<rules hidden>'];\n  return clone;\n}\nfunction safeStringify(v) {\n  try {\n    const seen = new WeakSet();\n    return JSON.stringify(v, (k, val) => {\n      if (typeof val === 'object' && val !== null) {\n        if (seen.has(val)) return '[Circular]';\n        seen.add(val);\n      }\n      return val;\n    });\n  } catch {\n    return '[unstringifiable]';\n  }\n}\nfunction deepMerge(a, b) {\n  if (b === null || b === undefined) return a;\n  if (Array.isArray(a) || Array.isArray(b) || typeof a !== 'object' || typeof b !== 'object') return b;\n  const out = { ...a };\n  for (const k of Object.keys(b)) out[k] = deepMerge(a[k], b[k]);\n  return out;\n}\n\n// ===== Levels & filtering =====\nconst LEVELS = ['debug', 'info', 'warn', 'error'];\nfunction levelAllows(min, lvl) {\n  const mi = LEVELS.indexOf((min || 'info').toLowerCase());\n  const li = LEVELS.indexOf((lvl || 'info').toLowerCase());\n  return li >= mi;\n}\n\n// ===== Active config (hot-reloadable) =====\nlet cfg = ENV_DEFAULTS;\nfunction loadFromEnv() { return JSON.parse(JSON.stringify(ENV_DEFAULTS)); }\n\nfunction validateConfig(c) {\n  const err = (m) => { const e = new Error(m); e.status = 400; throw e; };\n  if (!c || typeof c !== 'object') err('config must be an object');\n  if (c.level && !LEVELS.includes(c.level)) err(`invalid level: ${c.level}`);\n  if (c.sampling && typeof c.sampling.rate === 'number' && (c.sampling.rate < 0 || c.sampling.rate > 1))\n    err('sampling.rate must be between 0 and 1');\n  if (c.sinks?.hec?.enabled) {\n    const { url, token } = c.sinks.hec;\n    if (!url || !token) err('hec.url and hec.token are required when hec.enabled=true');\n  }\n  return c;\n}\n\nfunction getEffectiveLoggingConfig() {\n  return redact(cfg);\n}\n\nfunction setLoggingConfig(patch, { dryRun = false } = {}) {\n  if (patch && patch._reload) {\n    const reloaded = loadFromEnv();\n    validateConfig(reloaded);\n    if (!dryRun) cfg = reloaded;\n    return { applied: !dryRun, effective: redact(cfg) };\n    }\n  const next = validateConfig(deepMerge(cfg, patch));\n  if (dryRun) return { validated: true, next: redact(next) };\n  cfg = next;\n  return { applied: true, effective: redact(cfg) };\n}\n\n// ===== Sinks =====\nasync function sendConsole(entry) {\n  if (!cfg.console) return;\n  const line = `[${entry.ts}] [${entry.level.toUpperCase()}] ${entry.msg} ${entry.meta ? safeStringify(entry.meta) : ''}`;\n  try {\n    if (entry.level === 'debug') console.debug(line);\n    else if (entry.level === 'info') console.info(line);\n    else if (entry.level === 'warn') console.warn(line);\n    else console.error(line);\n  } catch {}\n}\n\nasync function sendHec(entry) {\n  const h = cfg.sinks?.hec || {};\n  if (!h.enabled) return;\n  const payload = {\n    event: {\n      level: entry.level,\n      message: entry.msg,\n      meta: entry.meta\n    },\n    time: Math.floor(Date.now() / 1000),\n    host: cfg.fields?.host || os.hostname(),\n    sourcetype: '_json',\n    source: h.source || 'llm-workflows',\n  };\n  if (h.index) payload.index = h.index;\n\n  const controller = new AbortController();\n  const timeout = setTimeout(() => controller.abort(), Number(h.timeout_ms || 3000));\n  try {\n    await fetch(h.url, {\n      method: 'POST',\n      headers: {\n        'Authorization': `Splunk ${h.token}`,\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify(payload),\n      signal: controller.signal\n    });\n  } finally {\n    clearTimeout(timeout);\n  }\n}\n\nconst sinks = [ sendConsole, sendHec ];\n\n// ===== Redaction of meta payloads =====\nfunction redactMeta(obj) {\n  if (!obj || typeof obj !== 'object') return obj;\n  const clone = JSON.parse(JSON.stringify(obj));\n  const REDACT_KEYS = ['authorization', 'Authorization', 'apiKey', 'token', 'password', 'secret'];\n  (function walk(o) {\n    if (!o || typeof o !== 'object') return;\n    for (const k of Object.keys(o)) {\n      if (REDACT_KEYS.includes(k)) o[k] = '***REDACTED***';\n      else if (typeof o[k] === 'object') walk(o[k]);\n    }\n  })(clone);\n  return clone;\n}\n\n// ===== Public log API =====\nfunction categoryLevel(min, category) {\n  const override = cfg.level_overrides?.[category];\n  return override || min;\n}\n\nfunction addLog(level, msg, meta, category) {\n  const min = categoryLevel(cfg.level, category);\n  if (!levelAllows(min, level)) return;\n\n  const rate = Number(cfg.sampling?.rate ?? 1);\n  if (rate < 1 && Math.random() > rate) return;\n\n  const baseMeta = { ...(meta || {}), service: cfg.fields?.service || 'llm-workflows' };\n  const entry = {\n    ts: new Date().toISOString(),\n    level,\n    msg: typeof msg === 'string' ? msg : safeStringify(msg),\n    meta: redactMeta(baseMeta),\n  };\n\n  pushBuffer(entry, cfg);\n\n  for (const sink of sinks) {\n    sink(entry).catch?.(() => {});\n  }\n}\n\nconst logDebug = (msg, meta, category)=> addLog('debug', msg, meta, category);\nconst logInfo  = (msg, meta, category)=> addLog('info',  msg, meta, category);\nconst logWarn  = (msg, meta, category)=> addLog('warn',  msg, meta, category);\nconst logError = (msg, meta, category)=> addLog('error', msg, meta, category);\n\nfunction safeStringifyPublic(v){ return safeStringify(v); }\n\nasync function testLoggingSink() {\n  const probe = { ts: Date.now(), probe: true, source: 'logging_test' };\n  logInfo('logging_test', probe, 'ops');\n  return { sent: true };\n}\n\nmodule.exports = {\n  logs,\n  stamp,\n  logDebug, logInfo, logWarn, logError,\n  safeStringify: safeStringifyPublic,\n  getEffectiveLoggingConfig,\n  setLoggingConfig,\n  testLoggingSink,\n};\n"
    },
    {
      "id": "add-logs-route",
      "description": "Add /api/logs endpoints for in-memory buffer access and clearing (parity with llm-gateway).",
      "op": "write_file",
      "path": "server/routes/logs.js",
      "mode": "overwrite",
      "content": "const express = require('express');\nconst router = express.Router();\nconst { logs } = require('../logger');\n\nrouter.get('/logs', (req, res) => {\n  const limit = Math.max(1, Math.min(Number(req.query.limit || 200), 2000));\n  const start = Math.max(0, logs.length - limit);\n  res.json(logs.slice(start));\n});\n\nrouter.post('/logs/clear', (_req, res) => {\n  logs.length = 0;\n  res.json({ ok: true });\n});\n\nmodule.exports = { router };\n"
    },
    {
      "id": "add-logging-route",
      "description": "Add runtime logging configuration endpoints (/api/logging) similar to llm-gateway with minimal internal auth.",
      "op": "write_file",
      "path": "server/routes/logging.js",
      "mode": "overwrite",
      "content": "const express = require('express');\nconst router = express.Router();\nconst {\n  getEffectiveLoggingConfig,\n  setLoggingConfig,\n  testLoggingSink,\n} = require('../logger');\n\nfunction requireInternalAuth(req, res, next) {\n  const token = process.env.INTERNAL_API_TOKEN;\n  if (!token) return next();\n  const hdr = req.headers['authorization'] || '';\n  if (hdr === `Bearer ${token}`) return next();\n  return res.status(401).json({ error: 'unauthorized' });\n}\n\nrouter.get('/logging', requireInternalAuth, (_req, res) => {\n  res.json({ effective: getEffectiveLoggingConfig(), redacted: true });\n});\n\nrouter.put('/logging', requireInternalAuth, (req, res) => {\n  const dry = String(req.query.dry_run || '').toLowerCase() === '1';\n  try {\n    const result = setLoggingConfig(req.body || {}, { dryRun: dry });\n    res.json({ ok: true, dry_run: dry, result });\n  } catch (e) {\n    res.status(400).json({ ok: false, error: e.message || 'invalid config' });\n  }\n});\n\nrouter.post('/logging/test', requireInternalAuth, async (_req, res) => {\n  try {\n    const r = await testLoggingSink();\n    res.json({ ok: true, result: r });\n  } catch (e) {\n    res.status(500).json({ ok: false, error: e.message || 'test failed' });\n  }\n});\n\nrouter.post('/logging/reload', requireInternalAuth, (_req, res) => {\n  try {\n    const result = setLoggingConfig({ _reload: true }, { dryRun: false });\n    res.json({ ok: true, result });\n  } catch (e) {\n    res.status(500).json({ ok: false, error: e.message || 'reload failed' });\n  }\n});\n\nmodule.exports = { router };\n"
    },
    {
      "id": "wire-logger-into-app",
      "description": "Use stamp middleware, add http access logging, mount /api/logs and /api/logging routes; add central error handler similar to gateway.",
      "op": "patch_text",
      "path": "server/app.js",
      "patches": [
        {
          "type": "replace_literal",
          "match": "const { logDebug, logInfo, logWarn, logError } = require('./logger');",
          "replacement": "const { stamp, logDebug, logInfo, logWarn, logError } = require('./logger');"
        },
        {
          "type": "insert_after",
          "anchor": "const Ajv = require('ajv');",
          "replacement": "const { router: logsRouter } = require('./routes/logs');\nconst { router: loggingRouter } = require('./routes/logging');\n"
        },
        {
          "type": "insert_after",
          "anchor": "app.use(bodyParser.json({ limit: '2mb' }));",
          "replacement": "// Attach request id to each request\napp.use(stamp);\n\n// Lightweight http access logging compatible with Splunk\napp.use((req, res, next) => {\n  const start = process.hrtime.bigint();\n  res.on('finish', () => {\n    const durMs = Number(process.hrtime.bigint() - start) / 1e6;\n    logInfo('http_access', {\n      rid: req.id,\n      method: req.method,\n      path: req.originalUrl || req.url,\n      status: res.statusCode,\n      duration_ms: Math.round(durMs),\n      ip: req.headers['x-forwarded-for'] || req.socket?.remoteAddress || 'unknown',\n      ua: req.headers['user-agent'] || ''\n    });\n  });\n  next();\n});\n"
        },
        {
          "type": "insert_after",
          "anchor": "app.use(express.static(pub));",
          "replacement": "// Log buffer and dynamic logging config\napp.use('/api', logsRouter);\napp.use('/api', loggingRouter);\n"
        },
        {
          "type": "insert_after",
          "anchor": "if (BASE_PATH) app.use(BASE_PATH, express.static(pub));",
          "replacement": "if (BASE_PATH) {\n  app.use(`${BASE_PATH}/api`, logsRouter);\n  app.use(`${BASE_PATH}/api`, loggingRouter);\n}\n"
        },
        {
          "type": "insert_after",
          "anchor": "if (BASE_PATH)\n  app.get(`${BASE_PATH}/health`, (_req, res) =>\n    res.json({ status: 'healthy', gatewayChatUrl: LLM_GATEWAY_CHAT_URL, gatewayApiBase: LLM_GATEWAY_API_BASE })\n  );",
          "replacement": "\n// Central error handler to ensure JSON + logging\napp.use((err, _req, res, _next) => {\n  try { logError('unhandled_error', { message: err?.message || String(err), stack: err?.stack }); } catch {}\n  res.status(500).json({ error: 'Internal Server Error' });\n});\n"
        }
      ]
    },
    {
      "id": "remove-legacy-splunk-logger-folder",
      "description": "Remove old splunk-logger shim folder since new logger has HEC sink built-in.",
      "op": "delete_path",
      "path": "splunk-logger",
      "recursive": true,
      "if_absent": "skip"
    }
  ]
}