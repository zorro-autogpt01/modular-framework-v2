{
  "version": "1.0",
  "backup": true,
  "changes": [
    {
      "": "gw-logger-upgrade",
      "description": "Replace llm-gateway logger with llm-chat-style logger: ring buffer, redaction, Splunk bridge, request stamping.",
      "op": "write_file",
      "path": "modular-framework/modules/llm-gateway/server/logger.js",
      "mode": "overwrite",
      "content": "const path = require('path');\nconst LOG_LEVEL = (process.env.LOG_LEVEL || 'info').toLowerCase();\nconst LOG_MAX = Number(process.env.LOG_MAX || 1000);\nconst LOG_TO_CONSOLE = (process.env.LOG_TO_CONSOLE || 'false').toLowerCase() === 'true';\nconst IS_SPLUNK_CONFIGURED = Boolean(process.env.SPLUNK_HEC_URL && process.env.SPLUNK_HEC_TOKEN);\n\n// Resolve splunk-logger helper from multiple candidate locations\nlet SPLUNK_LOGGER = null;\n(function resolveSplunkLogger(){\n  const candidates = [\n    '/splunk-logger',\n    path.join(__dirname, '..', 'splunk-logger'),\n    path.join(__dirname, '..', '..', 'splunk-logger'),\n    path.join(__dirname, '..', '..', '..', 'splunk-logger')\n  ];\n  for (const modPath of candidates) {\n    try {\n      SPLUNK_LOGGER = require(modPath);\n      break;\n    } catch (e) { /* continue */ }\n  }\n})();\n\nconst logs = [];\nlet reqCounter = 0;\n\nfunction redact(obj) {\n  if (!obj || typeof obj !== 'object') return obj;\n  const clone = JSON.parse(JSON.stringify(obj));\n  if (clone.apiKey) clone.apiKey = '***REDACTED***';\n  if (clone.headers && clone.headers.Authorization) clone.headers.Authorization = '***REDACTED***';\n  if (clone.headers && clone.headers.authorization) clone.headers.authorization = '***REDACTED***';\n  if (clone.Authorization) clone.Authorization = '***REDACTED***';\n  if (clone.authorization) clone.authorization = '***REDACTED***';\n  return clone;\n}\nfunction safeStringify(v) {\n  try {\n    const seen = new WeakSet();\n    return JSON.stringify(v, (k, val) => {\n      if (typeof val === 'object' && val !== null) {\n        if (seen.has(val)) return '[Circular]';\n        seen.add(val);\n      }\n      return val;\n    });\n  } catch {\n    return '[unstringifiable]';\n  }\n}\n\nfunction shouldConsole(level){\n  if (LOG_TO_CONSOLE) return true;\n  // If Splunk isn't configured, still emit to console so we don't go dark\n  return !IS_SPLUNK_CONFIGURED;\n}\nfunction consoleOut(level, line){\n  try {\n    if (!shouldConsole(level)) return;\n    if (level === 'debug' && LOG_LEVEL === 'debug') console.debug(line);\n    else if (level === 'info' && (LOG_LEVEL === 'debug' || LOG_LEVEL === 'info')) console.info(line);\n    else if (level === 'warn' && (LOG_LEVEL !== 'error')) console.warn(line);\n    else if (level === 'error') console.error(line);\n  } catch {}\n}\n\nfunction addLog(level, msg, meta) {\n  const entry = { ts: new Date().toISOString(), level, msg, ...meta };\n  logs.push(entry);\n  if (logs.length > LOG_MAX) logs.shift();\n  const line = `[${entry.ts}] [${level.toUpperCase()}] ${msg} ${meta ? safeStringify(meta) : ''}`;\n  consoleOut(level, line);\n}\n\nfunction augmentMeta(meta){\n  const base = (meta && typeof meta === 'object') ? meta : {};\n  return { service: 'llm-gateway', ...base };\n}\n\nconst logDebug = (msg, meta)=> { const m = augmentMeta(meta); addLog('debug', msg, m); try { SPLUNK_LOGGER?.logDebug?.(msg, m); } catch {} };\nconst logInfo  = (msg, meta)=> { const m = augmentMeta(meta); addLog('info', msg, m);  try { SPLUNK_LOGGER?.logInfo?.(msg, m); }  catch {} };\nconst logWarn  = (msg, meta)=> { const m = augmentMeta(meta); addLog('warn', msg, m);  try { SPLUNK_LOGGER?.logWarn?.(msg, m); }  catch {} };\nconst logError = (msg, meta)=> { const m = augmentMeta(meta); addLog('error', msg, m); try { SPLUNK_LOGGER?.logError?.(msg, m); } catch {} };\n\nfunction stamp(req, _res, next) {\n  req.id = `${Date.now().toString(36)}-${(++reqCounter).toString(36)}`;\n  next();\n}\n\nmodule.exports = { logs, redact, safeStringify, addLog, logDebug, logInfo, logWarn, logError, stamp };"
    },
    {
      "id": "gw-splunk-logger-upd",
      "description": "Align llm-gateway Splunk bridge with llm-chat (support SPLUNK_SOURCE and optional index).",
      "op": "write_file",
      "path": "modular-framework/modules/llm-gateway/splunk-logger/server/index.js",
      "mode": "overwrite",
      "content": "const os = require('os');\n\nconst SPLUNK_HEC_URL = process.env.SPLUNK_HEC_URL;\nconst SPLUNK_HEC_TOKEN = process.env.SPLUNK_HEC_TOKEN;\nconst SPLUNK_SOURCE = process.env.SPLUNK_SOURCE || 'llm-gateway';\nconst SPLUNK_INDEX = process.env.SPLUNK_INDEX || undefined; // optional\n\nconst configured = !!(SPLUNK_HEC_URL && SPLUNK_HEC_TOKEN);\n\nasync function logEvent(level, msg, meta) {\n  if (!configured) return;\n  const payload = {\n    event: {\n      level,\n      message: typeof msg === 'string' ? msg : JSON.stringify(msg),\n      meta\n    },\n    time: Math.floor(Date.now() / 1000),\n    host: os.hostname(),\n    sourcetype: '_json',\n    source: SPLUNK_SOURCE\n  };\n  if (SPLUNK_INDEX) payload.index = SPLUNK_INDEX;\n  try {\n    await fetch(SPLUNK_HEC_URL, {\n      method: 'POST',\n      headers: {\n        'Authorization': `Splunk ${SPLUNK_HEC_TOKEN}`,\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify(payload)\n    });\n  } catch (e) { /* ignore logging failures */ }\n}\n\nfunction logDebug(msg, meta){ return logEvent('debug', msg, meta); }\nfunction logInfo(msg, meta){ return logEvent('info', msg, meta); }\nfunction logWarn(msg, meta){ return logEvent('warn', msg, meta); }\nfunction logError(msg, meta){ return logEvent('error', msg, meta); }\n\nmodule.exports = { logDebug, logInfo, logWarn, logError };"
    },
    {
      "id": "gw-add-logs-route",
      "description": "Add /api/logs and /api/logs/clear endpoints to inspect in-memory logs from llm-gateway.",
      "op": "write_file",
      "path": "modular-framework/modules/llm-gateway/server/routes/logs.js",
      "mode": "create_if_missing",
      "content": "const express = require('express');\nconst router = express.Router();\nconst { logs } = require('../logger');\n\nrouter.get('/logs', (req, res) => {\n  const limit = Math.max(1, Math.min(Number(req.query.limit || 200), 2000));\n  const start = Math.max(0, logs.length - limit);\n  res.json(logs.slice(start));\n});\n\nrouter.post('/logs/clear', (_req, res) => {\n  logs.length = 0;\n  res.json({ ok: true });\n});\n\nmodule.exports = { router };"
    },
    {
      "id": "gw-app-stamp-access-logs",
      "description": "Use stamp middleware, add http access logging, mount logs routes, and add central error handler.",
      "op": "patch_text",
      "path": "modular-framework/modules/llm-gateway/server/app.js",
      "patches": [
        {
          "type": "replace_literal",
          "match": "const { initDb } = require('./db');",
          "replacement": "const { initDb } = require('./db');\nconst { router: logsRouter } = require('./routes/logs');\nconst { stamp, logInfo, logError } = require('./logger');"
        },
        {
          "type": "insert_after",
          "match": "app.use(bodyParser.json({ limit: '2mb' }));",
          "replacement": "\n// Attach request id to each request\napp.use(stamp);\n\n// Lightweight http access logging for Splunk\napp.use((req, res, next) => {\n  const start = process.hrtime.bigint();\n  res.on('finish', () => {\n    const durMs = Number(process.hrtime.bigint() - start) / 1e6;\n    logInfo('http_access', {\n      rid: req.id,\n      method: req.method,\n      path: req.originalUrl || req.url,\n      status: res.statusCode,\n      duration_ms: Math.round(durMs),\n      ip: req.headers['x-forwarded-for'] || req.socket?.remoteAddress || 'unknown',\n      ua: req.headers['user-agent'] || ''\n    });\n  });\n  next();\n});"
        },
        {
          "type": "insert_after",
          "match": "// Admin API",
          "replacement": "// Log buffer API\napp.use('/api', logsRouter);\n\n// Admin API"
        },
        {
          "type": "insert_after",
          "match": "app.use('/api', tokensRouter);       // /api/tokens",
          "replacement": "\n// Central error handler (ensures JSON + logs)\napp.use((err, _req, res, _next) => {\n  try { logError('unhandled_error', { message: err?.message || String(err), stack: err?.stack }); } catch {}\n  res.status(500).json({ error: 'Internal Server Error' });\n});"
        },
        {
          "type": "insert_after",
          "match": "app.use(`${BASE_PATH}/api`, tokensRouter);",
          "replacement": "\n  app.use(`${BASE_PATH}/api`, logsRouter);\n\n  // Error handler for BASE_PATH-mounted routes as well\n  app.use((err, _req, res, _next) => {\n    try { logError('unhandled_error', { message: err?.message || String(err), stack: err?.stack }); } catch {}\n    res.status(500).json({ error: 'Internal Server Error' });\n  });"
        }
      ]
    },
    {
      "id": "gw-chat-route-add-rid-and-lifecycle",
      "description": "Enhance chat routes with request id, SSE lifecycle logs, pass rid to providers.",
      "op": "patch_text",
      "path": "modular-framework/modules/llm-gateway/server/routes/chat.js",
      "patches": [
        {
          "type": "replace_literal",
          "match": "function prepareSSE(res) {",
          "replacement": "function prepareSSE(res, rid) {"
        },
        {
          "type": "replace_literal",
          "match": "  return (payload) => {",
          "replacement": "  return (payload) => {"
        },
        {
          "type": "replace_literal",
          "match": "      logDebug('GW -> client SSE', { payload });",
          "replacement": "      logDebug('GW -> client SSE', { rid, payload });"
        },
        {
          "type": "replace_literal",
          "match": "async function dispatch(modelRow, reqBody, res, sse) {",
          "replacement": "async function dispatch(modelRow, reqBody, res, sse, rid) {"
        },
        {
          "type": "replace_literal",
          "match": "  logInfo('GW LLM request', {",
          "replacement": "  logInfo('GW LLM request', { rid,"
        },
        {
          "type": "replace_literal",
          "match": "  logDebug('GW upstream payload', { upstream });",
          "replacement": "  logDebug('GW upstream payload', { rid, upstream });"
        },
        {
          "type": "replace_literal",
          "match": "  const onDelta = (d) => {",
          "replacement": "  const onDelta = (d) => {"
        },
        {
          "type": "replace_literal",
          "match": "    logDebug('GW upstream delta', { len: typeof d === 'string' ? d.length : 0, data: d });",
          "replacement": "    logDebug('GW upstream delta', { rid, len: typeof d === 'string' ? d.length : 0, data: d });"
        },
        {
          "type": "replace_literal",
          "match": "  const onDone = () => { logDebug('GW upstream done'); sse?.({ type: 'done' }); };",
          "replacement": "  const onDone = () => { logDebug('GW upstream done', { rid }); sse?.({ type: 'done' }); };"
        },
        {
          "type": "replace_literal",
          "match": "  const onError = (m) => { logWarn('GW upstream error', { message: m }); sse?.({ type: 'error', message: m }); };",
          "replacement": "  const onError = (m) => { logWarn('GW upstream error', { rid, message: m }); sse?.({ type: 'error', message: m }); };"
        },
        {
          "type": "replace_literal",
          "match": "      await callOllama({ ...upstream, onDelta, onDone, onError });",
          "replacement": "      await callOllama({ ...upstream, onDelta, onDone, onError, rid });"
        },
        {
          "type": "replace_literal",
          "match": "      const { content } = await callOllama({ ...upstream, stream:false });",
          "replacement": "      const { content } = await callOllama({ ...upstream, stream:false, rid });"
        },
        {
          "type": "replace_literal",
          "match": "    await callOpenAICompat({",
          "replacement": "    await callOpenAICompat({"
        },
        {
          "type": "replace_literal",
          "match": "      ...upstream,",
          "replacement": "      ...upstream, rid,"
        },
        {
          "type": "replace_literal",
          "match": "    const { content } = await callOpenAICompat({",
          "replacement": "    const { content } = await callOpenAICompat({"
        },
        {
          "type": "replace_literal",
          "match": "      ...upstream, stream: false",
          "replacement": "      ...upstream, stream: false, rid"
        },
        {
          "type": "replace_literal",
          "match": "router.post('/v1/chat', async (req, res) => {",
          "replacement": "router.post('/v1/chat', async (req, res) => {"
        },
        {
          "type": "insert_after",
          "match": "router.post('/v1/chat', async (req, res) => {",
          "replacement": "  const rid = req.id;\n"
        },
        {
          "type": "replace_literal",
          "match": "  const sse = stream ? prepareSSE(res) : null;",
          "replacement": "  const sse = stream ? prepareSSE(res, rid) : null;"
        },
        {
          "type": "replace_literal",
          "match": "  logInfo('GW /api/v1/chat <- client', {",
          "replacement": "  logInfo('GW /api/v1/chat <- client', { rid,"
        },
        {
          "type": "insert_after",
          "match": "  try {",
          "replacement": "  // Track SSE connection lifecycle\n  let ended = false;\n  const onEnd = (kind) => { if (ended) return; ended = true; logInfo('GW /v1/chat finished', { rid, kind }); };\n  res.on('close', () => onEnd('close'));\n  res.on('finish', () => onEnd('finish'));\n\n  try {"
        },
        {
          "type": "replace_literal",
          "match": "    if (stream) await dispatch(modelRow, req.body, res, sse);",
          "replacement": "    if (stream) await dispatch(modelRow, req.body, res, sse, rid);"
        },
        {
          "type": "replace_literal",
          "match": "    else await dispatch(modelRow, req.body, res, null);",
          "replacement": "    else await dispatch(modelRow, req.body, res, null, rid);"
        },
        {
          "type": "replace_literal",
          "match": "    logError('GW /v1/chat error', { err: err?.message || String(err) });",
          "replacement": "    logError('GW /v1/chat error', { rid, err: err?.message || String(err) });"
        },
        {
          "type": "replace_literal",
          "match": "router.post('/compat/llm-chat', async (req, res) => {",
          "replacement": "router.post('/compat/llm-chat', async (req, res) => {"
        },
        {
          "type": "insert_after",
          "match": "router.post('/compat/llm-chat', async (req, res) => {",
          "replacement": "  const rid = req.id;\n"
        },
        {
          "type": "replace_literal",
          "match": "  const sse = stream ? prepareSSE(res) : null;",
          "replacement": "  const sse = stream ? prepareSSE(res, rid) : null;"
        },
        {
          "type": "replace_literal",
          "match": "  logInfo('GW /api/compat/llm-chat <- client', {",
          "replacement": "  logInfo('GW /api/compat/llm-chat <- client', { rid,"
        },
        {
          "type": "insert_after",
          "match": "  try {",
          "replacement": "  let ended = false; const onEnd = (k)=>{ if(ended) return; ended=true; logInfo('GW /compat/llm-chat finished', { rid, kind:k }); };\n  res.on('close',()=>onEnd('close')); res.on('finish',()=>onEnd('finish'));\n  try {"
        },
        {
          "type": "replace_literal",
          "match": "    if (stream) await dispatch(modelRow, req.body, res, sse);",
          "replacement": "    if (stream) await dispatch(modelRow, req.body, res, sse, rid);"
        },
        {
          "type": "replace_literal",
          "match": "    else await dispatch(modelRow, req.body, res, null);",
          "replacement": "    else await dispatch(modelRow, req.body, res, null, rid);"
        },
        {
          "type": "replace_literal",
          "match": "    logError('GW /compat/llm-chat error', { err: err?.message || String(err) });",
          "replacement": "    logError('GW /compat/llm-chat error', { rid, err: err?.message || String(err) });"
        },
        {
          "type": "replace_literal",
          "match": "router.post('/compat/llm-workflows', async (req, res) => {",
          "replacement": "router.post('/compat/llm-workflows', async (req, res) => {"
        },
        {
          "type": "insert_after",
          "match": "router.post('/compat/llm-workflows', async (req, res) => {",
          "replacement": "  const rid = req.id;\n"
        },
        {
          "type": "replace_literal",
          "match": "  const write = stream ? prepareSSE(res) : null;",
          "replacement": "  const write = stream ? prepareSSE(res, rid) : null;"
        },
        {
          "type": "replace_literal",
          "match": "  logInfo('GW /api/compat/llm-workflows <- workflows', {",
          "replacement": "  logInfo('GW /api/compat/llm-workflows <- workflows', { rid,"
        },
        {
          "type": "replace_literal",
          "match": "      write({ type: 'llm.delta', data: payload.content });",
          "replacement": "      write({ type: 'llm.delta', data: payload.content, rid });"
        },
        {
          "type": "replace_literal",
          "match": "      write({ type: 'done' });",
          "replacement": "      write({ type: 'done', rid });"
        },
        {
          "type": "replace_literal",
          "match": "      write({ type: 'error', message: payload.message });",
          "replacement": "      write({ type: 'error', message: payload.message, rid });"
        },
        {
          "type": "insert_after",
          "match": "  try {",
          "replacement": "  let ended = false; const onEnd = (k)=>{ if(ended) return; ended=true; logInfo('GW /compat/llm-workflows finished', { rid, kind:k }); };\n  res.on('close',()=>onEnd('close')); res.on('finish',()=>onEnd('finish'));\n  try {"
        },
        {
          "type": "replace_literal",
          "match": "    if (stream) await dispatch(modelRow, req.body, res, sse);",
          "replacement": "    if (stream) await dispatch(modelRow, req.body, res, sse, rid);"
        },
        {
          "type": "replace_literal",
          "match": "    else await dispatch(modelRow, req.body, res, null);",
          "replacement": "    else await dispatch(modelRow, req.body, res, null, rid);"
        },
        {
          "type": "replace_literal",
          "match": "    logError('GW /compat/llm-workflows error', { err: err?.message || String(err) });",
          "replacement": "    logError('GW /compat/llm-workflows error', { rid, err: err?.message || String(err) });"
        }
      ]
    },
    {
      "id": "gw-provider-openai-pass-rid",
      "description": "Add rid to OpenAI-compatible provider and include in logs.",
      "op": "patch_text",
      "path": "modular-framework/modules/llm-gateway/server/providers/openaiCompat.js",
      "patches": [
        {
          "type": "replace_literal",
          "match": "async function callOpenAICompat({",
          "replacement": "async function callOpenAICompat({"
        },
        {
          "type": "replace_literal",
          "match": "  baseUrl, apiKey, model, messages, temperature,",
          "replacement": "  baseUrl, apiKey, model, messages, temperature,"
        },
        {
          "type": "replace_literal",
          "match": "  max_tokens, useResponses, reasoning, stream, onDelta, onDone, onError",
          "replacement": "  max_tokens, useResponses, reasoning, stream, onDelta, onDone, onError, rid"
        },
        {
          "type": "replace_literal",
          "match": "    logDebug('GW RESPONSES request', { url, model, stream, headers: redactHeaders(headers) });",
          "replacement": "    logDebug('GW RESPONSES request', { rid, url, model, stream, headers: redactHeaders(headers) });"
        },
        {
          "type": "replace_literal",
          "match": "    logDebug('GW RESPONSES request body', { body });",
          "replacement": "    logDebug('GW RESPONSES request body', { rid, body });"
        },
        {
          "type": "replace_literal",
          "match": "      logDebug('GW RESPONSES streaming started', { status: response.status });",
          "replacement": "      logDebug('GW RESPONSES streaming started', { rid, status: response.status });"
        },
        {
          "type": "replace_literal",
          "match": "        response.data.on('end', () => { logDebug('GW RESPONSES stream end'); onDone?.(); resolve(); });",
          "replacement": "        response.data.on('end', () => { logDebug('GW RESPONSES stream end', { rid }); onDone?.(); resolve(); });"
        },
        {
          "type": "replace_literal",
          "match": "        response.data.on('error', (e) => { logWarn('GW RESPONSES stream error', { message: e.message }); onError?.(e.message); resolve(); });",
          "replacement": "        response.data.on('error', (e) => { logWarn('GW RESPONSES stream error', { rid, message: e.message }); onError?.(e.message); resolve(); });"
        },
        {
          "type": "replace_literal",
          "match": "      logDebug('GW RESPONSES non-stream response', {",
          "replacement": "      logDebug('GW RESPONSES non-stream response', { rid,"
        },
        {
          "type": "replace_literal",
          "match": "  logDebug('GW CHAT request', { url, model, stream, headers: redactHeaders(headers) });",
          "replacement": "  logDebug('GW CHAT request', { rid, url, model, stream, headers: redactHeaders(headers) });"
        },
        {
          "type": "replace_literal",
          "match": "  logDebug('GW CHAT request body', { body });",
          "replacement": "  logDebug('GW CHAT request body', { rid, body });"
        },
        {
          "type": "replace_literal",
          "match": "    logDebug('GW CHAT streaming started', { status: response.status });",
          "replacement": "    logDebug('GW CHAT streaming started', { rid, status: response.status });"
        },
        {
          "type": "replace_literal",
          "match": "      response.data.on('end', () => { logDebug('GW CHAT stream end'); onDone?.(); resolve(); });",
          "replacement": "      response.data.on('end', () => { logDebug('GW CHAT stream end', { rid }); onDone?.(); resolve(); });"
        },
        {
          "type": "replace_literal",
          "match": "      response.data.on('error', (e) => { logWarn('GW CHAT stream error', { message: e.message }); onError?.(e.message); resolve(); });",
          "replacement": "      response.data.on('error', (e) => { logWarn('GW CHAT stream error', { rid, message: e.message }); onError?.(e.message); resolve(); });"
        },
        {
          "type": "replace_literal",
          "match": "  logDebug('GW CHAT non-stream response', {",
          "replacement": "  logDebug('GW CHAT non-stream response', { rid,"
        }
      ]
    },
    {
      "id": "gw-provider-ollama-pass-rid",
      "description": "Add rid to Ollama provider and include in logs.",
      "op": "patch_text",
      "path": "modular-framework/modules/llm-gateway/server/providers/ollama.js",
      "patches": [
        {
          "type": "replace_literal",
          "match": "async function callOllama({ baseUrl, model, messages, temperature, stream, onDelta, onDone, onError }) {",
          "replacement": "async function callOllama({ baseUrl, model, messages, temperature, stream, onDelta, onDone, onError, rid }) {"
        },
        {
          "type": "replace_literal",
          "match": "  logDebug('GW OLLAMA request', { url, model, stream });",
          "replacement": "  logDebug('GW OLLAMA request', { rid, url, model, stream });"
        },
        {
          "type": "replace_literal",
          "match": "  logDebug('GW OLLAMA request body', { body });",
          "replacement": "  logDebug('GW OLLAMA request body', { rid, body });"
        },
        {
          "type": "replace_literal",
          "match": "    logDebug('GW OLLAMA streaming started', { status: response.status });",
          "replacement": "    logDebug('GW OLLAMA streaming started', { rid, status: response.status });"
        },
        {
          "type": "replace_literal",
          "match": "        logDebug('GW OLLAMA stream chunk', { size: Buffer.byteLength(str), raw: str.slice(0, 800) });",
          "replacement": "        logDebug('GW OLLAMA stream chunk', { rid, size: Buffer.byteLength(str), raw: str.slice(0, 800) });"
        },
        {
          "type": "replace_literal",
          "match": "            logDebug('GW OLLAMA stream line parsed', { evt });",
          "replacement": "            logDebug('GW OLLAMA stream line parsed', { rid, evt });"
        },
        {
          "type": "replace_literal",
          "match": "            logWarn('GW OLLAMA stream line parse error', { message: e.message, line: line.slice(0, 300) });",
          "replacement": "            logWarn('GW OLLAMA stream line parse error', { rid, message: e.message, line: line.slice(0, 300) });"
        },
        {
          "type": "replace_literal",
          "match": "      response.data.on('end', () => { logDebug('GW OLLAMA stream end'); onDone?.(); resolve(); });",
          "replacement": "      response.data.on('end', () => { logDebug('GW OLLAMA stream end', { rid }); onDone?.(); resolve(); });"
        },
        {
          "type": "replace_literal",
          "match": "      response.data.on('error', (e) => { logWarn('GW OLLAMA stream error', { message: e.message }); onError?.(e.message); resolve(); });",
          "replacement": "      response.data.on('error', (e) => { logWarn('GW OLLAMA stream error', { rid, message: e.message }); onError?.(e.message); resolve(); });"
        },
        {
          "type": "replace_literal",
          "match": "    logDebug('GW OLLAMA non-stream response', {",
          "replacement": "    logDebug('GW OLLAMA non-stream response', { rid,"
        }
      ]
    },
    {
      "id": "gw-docker-compose-enable-splunk",
      "description": "Enable Splunk env for llm-gateway service and align defaults with llm-chat for consistent logging.",
      "op": "patch_text",
      "path": "modular-framework/docker-compose.yml",
      "patches": [
        {
          "type": "replace_literal",
          "match": "  llm-gateway:\n    build: ./modules/llm-gateway\n    container_name: llm-gateway\n    env_file: ./.env\n    environment:",
          "replacement": "  llm-gateway:\n    build: ./modules/llm-gateway\n    container_name: llm-gateway\n    env_file: ./.env\n    environment:"
        },
        {
          "type": "replace_literal",
          "match": "      - BASE_PATH=/llm-gateway",
          "replacement": "      - BASE_PATH=/llm-gateway"
        },
        {
          "type": "replace_literal",
          "match": "      #- SPLUNK_HEC_TOKEN=$SPLUNK_HEC_TOKEN",
          "replacement": "      - SPLUNK_HEC_TOKEN=${SPLUNK_HEC_TOKEN}"
        },
        {
          "type": "replace_literal",
          "match": "      #- SPLUNK_HEC_URL=$SPLUNK_HEC_URL",
          "replacement": "      - SPLUNK_HEC_URL=${SPLUNK_HEC_URL}"
        },
        {
          "type": "insert_after",
          "match": "      - SPLUNK_HEC_URL=${SPLUNK_HEC_URL}",
          "replacement": "      - SPLUNK_SOURCE=llm-gateway\n      - LOG_TO_CONSOLE=false"
        }
      ]
    }
  ]
}