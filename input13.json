{
  "version": "1.0",
  "project_root": "",
  "dry_run": false,
  "backup": true,
  "changes": [
    {
      "id": "llm-documentor-engine-01",
      "description": "Create modular engine package (github_hub, extractor, normalizer, generator, verifier) to split big app.py responsibilities into smaller modules.",
      "op": "write_file",
      "path": "modular-framework/modules/llm-documentor/engine/__init__.py",
      "content": "'''Engine package initialization for llm-documentor - re-exports core classes.'''\n\nfrom .github_hub import fetch_repo_tree, fetch_file_content\nfrom .extractor import CodeExtractor\nfrom .normalizer import ChunkNormalizer\nfrom .generator import DocGenerator\nfrom .verifier import DocVerifier\n\n__all__ = [\n    'fetch_repo_tree', 'fetch_file_content',\n    'CodeExtractor', 'ChunkNormalizer', 'DocGenerator', 'DocVerifier'\n]\n",
      "mode": "create_new"
    },
    {
      "id": "llm-documentor-engine-02",
      "description": "Move GitHub Hub HTTP helpers into engine/github_hub.py (preserve original behavior and error handling).",
      "op": "write_file",
      "path": "modular-framework/modules/llm-documentor/engine/github_hub.py",
      "content": "import os\nimport aiohttp\nfrom fastapi import HTTPException\n\nDEFAULT_GITHUB_HUB = os.getenv('GITHUB_HUB_URL', 'http://github-hub-module:3005/api')\n\nasync def fetch_repo_tree(repo_url: str, branch: str, base_url: str | None = None) -> dict:\n    \"\"\"Fetch repository tree from the GitHub Hub service.\n\n    Note: kept signature compatible with the original app (repo_url arg is accepted\n    to preserve call sites; the underlying hub API previously only used branch).\n    \"\"\"\n    base = (base_url or DEFAULT_GITHUB_HUB).rstrip('/')\n    params = {\"branch\": branch, \"recursive\": \"true\"}\n    async with aiohttp.ClientSession() as session:\n        async with session.get(f\"{base}/tree\", params=params) as resp:\n            if resp.status != 200:\n                raise HTTPException(status_code=resp.status, detail=\"Failed to fetch repo tree\")\n            return await resp.json()\n\n\nasync def fetch_file_content(path: str, branch: str, base_url: str | None = None) -> str:\n    \"\"\"Fetch a file's content from the GitHub Hub service.\n\n    Returns decoded_content or empty string on non-200 like the original code.\n    \"\"\"\n    base = (base_url or DEFAULT_GITHUB_HUB).rstrip('/')\n    params = {\"path\": path, \"branch\": branch}\n    async with aiohttp.ClientSession() as session:\n        async with session.get(f\"{base}/file\", params=params) as resp:\n            if resp.status != 200:\n                return \"\"\n            data = await resp.json()\n            return data.get(\"decoded_content\", \"\")\n",
      "mode": "create_new"
    },
    {
      "id": "llm-documentor-engine-03",
      "description": "Extract CodeExtractor class into engine/extractor.py. It accepts cache_dir and defers GitHub requests to engine.github_hub to keep app.py lean.",
      "op": "write_file",
      "path": "modular-framework/modules/llm-documentor/engine/extractor.py",
      "content": "import hashlib\nimport json\nimport re\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import List, Dict, Any\n\nfrom .github_hub import fetch_repo_tree, fetch_file_content\n\n\nclass CodeExtractor:\n    \"\"\"Extract structured information from a repository.\n\n    This replicates the prior behavior but takes explicit cache_dir and (optionally)\n    a github_base_url via the github_hub helpers.\n    \"\"\"\n\n    def __init__(self, repo_url: str, branch: str, cache_dir: str | Path):\n        self.repo_url = repo_url\n        self.branch = branch\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n        self.cache_key = hashlib.sha256(f\"{repo_url}:{branch}\".encode()).hexdigest()\n\n    async def extract(self) -> Dict[str, Any]:\n        \"\"\"Run extraction and return artifacts (structure, files, api_specs, schemas, tests).\"\"\"\n        cache_file = self.cache_dir / f\"{self.cache_key}.json\"\n        if cache_file.exists():\n            with cache_file.open('r', encoding='utf-8') as f:\n                return json.load(f)\n\n        tree = await fetch_repo_tree(self.repo_url, self.branch)\n        items = tree.get('items', [])\n\n        artifacts = {\n            'meta': {\n                'repo': self.repo_url,\n                'branch': self.branch,\n                'extracted_at': datetime.utcnow().isoformat()\n            },\n            'structure': self._build_structure(items),\n            'files': {},\n            'api_specs': {},\n            'configs': {},\n            'schemas': {},\n            'tests': []\n        }\n\n        for item in items:\n            if item.get('type') != 'blob':\n                continue\n            path = item.get('path')\n\n            # YAML/JSON candidates for API specs\n            if re.match(r\".*\\.(yaml|yml|json)$\", path, re.I):\n                if 'openapi' in path.lower() or 'swagger' in path.lower():\n                    content = await fetch_file_content(path, self.branch)\n                    artifacts['api_specs'][path] = self._parse_openapi(content)\n\n            elif path.endswith('.sql'):\n                content = await fetch_file_content(path, self.branch)\n                artifacts['schemas'][path] = content\n\n            elif 'docker' in path.lower() or path.endswith('docker-compose.yml'):\n                content = await fetch_file_content(path, self.branch)\n                artifacts['configs'][path] = content\n\n            elif 'test' in path.lower() or 'spec' in path.lower():\n                artifacts['tests'].append(path)\n\n            elif self._is_key_source_file(path):\n                content = await fetch_file_content(path, self.branch)\n                artifacts['files'][path] = {\n                    'content': content,\n                    'language': self._detect_language(path),\n                    'symbols': self._extract_symbols(content, path)\n                }\n\n        with cache_file.open('w', encoding='utf-8') as f:\n            json.dump(artifacts, f, indent=2)\n\n        return artifacts\n\n    def _build_structure(self, items: List[Dict]) -> Dict:\n        root = {'type': 'dir', 'children': {}}\n        for item in items:\n            parts = item.get('path', '').split('/')\n            current = root\n            for i, part in enumerate(parts):\n                is_leaf = (i == len(parts) - 1)\n                if is_leaf:\n                    if item.get('type') == 'blob':\n                        current['children'][part] = {'type': 'file', 'size': item.get('size', 0)}\n                    else:\n                        current['children'][part] = {'type': 'dir', 'children': {}}\n                else:\n                    if part not in current['children']:\n                        current['children'][part] = {'type': 'dir', 'children': {}}\n                    current = current['children'][part]\n        return root\n\n    def _is_key_source_file(self, path: str) -> bool:\n        key_patterns = [\n            r\".*/(index|main|app|server)\\.(js|ts|py|go|java)$\",\n            r\".*/routes/.*\\.(js|ts|py)$\",\n            r\".*/models/.*\\.(js|ts|py)$\",\n            r\".*/api/.*\\.(js|ts|py)$\",\n        ]\n        return any(re.match(p, path) for p in key_patterns)\n\n    def _detect_language(self, path: str) -> str:\n        ext = path.split('.')[-1].lower()\n        mapping = {\n            'js': 'javascript', 'ts': 'typescript', 'py': 'python',\n            'go': 'go', 'java': 'java', 'rb': 'ruby', 'rs': 'rust'\n        }\n        return mapping.get(ext, 'unknown')\n\n    def _extract_symbols(self, content: str, path: str) -> List[str]:\n        symbols = []\n        lang = self._detect_language(path)\n        if lang in ['javascript', 'typescript']:\n            symbols.extend(re.findall(r\"(?:function|const|let|var|class)\\s+(\\w+)\", content))\n            symbols.extend(re.findall(r\"(\\w+)\\s*:\\s*(?:async\\s*)?\\(\", content))\n        elif lang == 'python':\n            symbols.extend(re.findall(r\"^(?:def|class)\\s+(\\w+)\", content, re.M))\n        # dedupe and cap\n        return list(dict.fromkeys(symbols))[:50]\n\n    def _parse_openapi(self, content: str) -> dict:\n        try:\n            import json, yaml\n            spec = None\n            if content.strip().startswith('{'):\n                spec = json.loads(content)\n            else:\n                spec = yaml.safe_load(content)\n            return {\n                'version': spec.get('openapi', spec.get('swagger', 'unknown')),\n                'paths': list(spec.get('paths', {}).keys()),\n                'schemas': list(spec.get('components', {}).get('schemas', {}).keys())\n            }\n        except Exception:\n            return {}\n",
      "mode": "create_new"
    },
    {
      "id": "llm-documentor-engine-04",
      "description": "Extract ChunkNormalizer into engine/normalizer.py (same behavior, formatting, chunking).",
      "op": "write_file",
      "path": "modular-framework/modules/llm-documentor/engine/normalizer.py",
      "content": "import json\nfrom typing import List, Dict, Any\n\n\nclass ChunkNormalizer:\n    \"\"\"Normalize and chunk artifacts for LLM consumption.\"\"\"\n\n    def __init__(self, artifacts: Dict[str, Any]):\n        self.artifacts = artifacts\n\n    def normalize(self) -> List[Dict[str, Any]]:\n        chunks: List[Dict[str, Any]] = []\n\n        # Structure overview\n        chunks.append({\n            'type': 'structure',\n            'content': self._format_structure(self.artifacts['structure']),\n            'metadata': {\n                'repo': self.artifacts['meta']['repo'],\n                'branch': self.artifacts['meta']['branch']\n            }\n        })\n\n        # API specs\n        for path, spec in self.artifacts.get('api_specs', {}).items():\n            chunks.append({'type': 'api_spec', 'path': path, 'content': json.dumps(spec, indent=2), 'metadata': {'version': spec.get('version')}})\n\n        # Source files\n        for path, file_info in self.artifacts.get('files', {}).items():\n            content = file_info.get('content', '')\n            if len(content) > 5000:\n                for i, sub in enumerate(self._smart_chunk(content, file_info.get('language'))):\n                    chunks.append({'type': 'source', 'path': path, 'part': i + 1, 'content': sub, 'language': file_info.get('language'), 'symbols': file_info.get('symbols')})\n            else:\n                chunks.append({'type': 'source', 'path': path, 'content': content, 'language': file_info.get('language'), 'symbols': file_info.get('symbols')})\n\n        for path, content in self.artifacts.get('schemas', {}).items():\n            chunks.append({'type': 'schema', 'path': path, 'content': content})\n\n        return chunks\n\n    def _format_structure(self, node: Dict, indent: int = 0) -> str:\n        lines = []\n        for name, child in node.get('children', {}).items():\n            prefix = '  ' * indent\n            if child['type'] == 'file':\n                lines.append(f\"{prefix}ðŸ“„ {name}\")\n            else:\n                lines.append(f\"{prefix}ðŸ“ {name}\")\n                lines.append(self._format_structure(child, indent + 1))\n        return '\\n'.join(lines)\n\n    def _smart_chunk(self, content: str, language: str, max_size: int = 3000) -> List[str]:\n        chunks = []\n        current = []\n        size = 0\n        lines = content.split('\\n')\n        for line in lines:\n            is_boundary = False\n            if language == 'python' and line.strip().startswith(('def ', 'class ')):\n                is_boundary = True\n            if language in ('javascript', 'typescript') and line.strip().startswith(('function ', 'class ', 'export ')):\n                is_boundary = True\n\n            if is_boundary and size > max_size // 2:\n                chunks.append('\\n'.join(current))\n                current = []\n                size = 0\n\n            current.append(line)\n            size += len(line)\n            if size > max_size:\n                chunks.append('\\n'.join(current))\n                current = []\n                size = 0\n\n        if current:\n            chunks.append('\\n'.join(current))\n        return chunks\n",
      "mode": "create_new"
    },
    {
      "id": "llm-documentor-engine-05",
      "description": "Move DocGenerator into engine/generator.py. Uses the templates package already present and performs identical LLM calls with configurable gateway URL fallback.",
      "op": "write_file",
      "path": "modular-framework/modules/llm-documentor/engine/generator.py",
      "content": "import os\nimport json\nfrom typing import List, Dict, Any\nimport aiohttp\n\nfrom templates import get_template\n\nDEFAULT_LLM_GATEWAY = os.getenv('LLM_GATEWAY_URL', 'http://llm-gateway:3010/api')\n\n\nclass DocGenerator:\n    \"\"\"Generate documentation using an LLM gateway.\n\n    Compatible with the original behavior; will use DEFAULT_LLM_GATEWAY if not passed.\n    \"\"\"\n\n    def __init__(self, chunks: List[Dict[str, Any]], model_key: str, llm_gateway_url: str | None = None):\n        self.chunks = chunks\n        self.model_key = model_key\n        self.llm_gateway_url = (llm_gateway_url or DEFAULT_LLM_GATEWAY).rstrip('/')\n\n    async def generate(self, pack, job_id: str) -> Dict[str, str]:\n        template = get_template(pack.template)\n        relevant_chunks = self._filter_chunks_for_pack(pack)\n        docs: Dict[str, str] = {}\n\n        if pack.name == 'api':\n            api_chunks = [c for c in relevant_chunks if c['type'] == 'api_spec']\n            for chunk in api_chunks:\n                doc = await self._call_llm(template, chunk)\n                endpoint_group = chunk['path'].split('/')[-1].replace('.yaml', '').replace('.json', '')\n                docs[f\"api/{endpoint_group}.md\"] = doc\n\n        elif pack.name in ('super-detailed', 'detailed'):\n            source_chunks = [c for c in relevant_chunks if c['type'] == 'source']\n            components: Dict[str, List[Dict]] = {}\n            for chunk in source_chunks:\n                component = self._extract_component_name(chunk['path'])\n                components.setdefault(component, []).append(chunk)\n            for component, comp_chunks in components.items():\n                context = self._merge_chunks(comp_chunks)\n                doc = await self._call_llm(template, context)\n                docs[f\"components/{component}.md\"] = doc\n        else:\n            context = self._merge_chunks(relevant_chunks)\n            doc = await self._call_llm(template, context)\n            docs[pack.output_path] = doc\n\n        return docs\n\n    def _filter_chunks_for_pack(self, pack) -> List[Dict]:\n        name = pack.name\n        if name == 'api':\n            return [c for c in self.chunks if c.get('type') in ('api_spec', 'source') and 'api' in c.get('path', '').lower()]\n        if name == 'db':\n            return [c for c in self.chunks if c.get('type') == 'schema']\n        if name in ('super-detailed', 'detailed'):\n            return [c for c in self.chunks if c.get('type') == 'source']\n        return self.chunks\n\n    def _merge_chunks(self, chunks: List[Dict]) -> str:\n        parts = []\n        for chunk in chunks[:10]:\n            t = chunk.get('type')\n            if t == 'source':\n                parts.append(f\"=== {chunk['path']} ===\\nLanguage: {chunk.get('language')}\\n{chunk.get('content')[:2000]}\\n\")\n            elif t == 'api_spec':\n                parts.append(f\"=== API Spec: {chunk['path']} ===\\n{chunk.get('content')[:2000]}\\n\")\n            elif t == 'schema':\n                parts.append(f\"=== Schema: {chunk['path']} ===\\n{chunk.get('content')[:2000]}\\n\")\n            elif t == 'structure':\n                parts.append(f\"=== Repository Structure ===\\n{chunk.get('content')[:1000]}\\n\")\n        return '\\n'.join(parts)\n\n    def _extract_component_name(self, path: str) -> str:\n        parts = path.split('/')\n        if 'modules' in parts:\n            idx = parts.index('modules')\n            if idx + 1 < len(parts):\n                return parts[idx + 1]\n        if 'src' in parts:\n            idx = parts.index('src')\n            if idx + 1 < len(parts):\n                return parts[idx + 1]\n        return parts[-2] if len(parts) > 1 else 'main'\n\n    async def _call_llm(self, template: str, context: Any) -> str:\n        if isinstance(context, dict):\n            context_str = json.dumps(context, indent=2)[:8000]\n        else:\n            context_str = str(context)[:8000]\n        prompt = template.format(context=context_str)\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a technical documentation expert. Generate comprehensive, accurate documentation based on the provided code context.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n        payload = {\"modelKey\": self.model_key, \"messages\": messages, \"temperature\": 0.3, \"max_tokens\": 4000, \"stream\": False}\n\n        async with aiohttp.ClientSession() as session:\n            async with session.post(f\"{self.llm_gateway_url}/v1/chat\", json=payload) as resp:\n                if resp.status != 200:\n                    error_text = await resp.text()\n                    return f\"# Documentation Generation Failed\\n\\nError: {error_text}\"\n                result = await resp.json()\n                return result.get('content', '# No content generated')\n",
      "mode": "create_new"
    },
    {
      "id": "llm-documentor-engine-06",
      "description": "Move DocVerifier to engine/verifier.py to keep verification logic isolated.",
      "op": "write_file",
      "path": "modular-framework/modules/llm-documentor/engine/verifier.py",
      "content": "import re\nfrom typing import Dict, Any\n\n\nclass DocVerifier:\n    \"\"\"Verify generated documentation quality.\n\n    Same checks as original: length, placeholders, header, file references.\n    \"\"\"\n\n    def __init__(self, docs: Dict[str, str], artifacts: Dict):\n        self.docs = docs\n        self.artifacts = artifacts\n\n    async def verify(self) -> Dict[str, Any]:\n        results = {'checks': [], 'warnings': [], 'errors': []}\n        for path, content in self.docs.items():\n            if len(content) < 100:\n                results['errors'].append(f\"{path}: Documentation too short\")\n            if '[TODO]' in content or 'FIXME' in content:\n                results['warnings'].append(f\"{path}: Contains placeholders\")\n            if not content.startswith('#'):\n                results['warnings'].append(f\"{path}: Missing header\")\n            file_refs = re.findall(r\"`([^`]+\\.(py|js|ts|go))`\", content)\n            for ref, _ext in file_refs:\n                if ref not in self.artifacts.get('files', {}):\n                    results['warnings'].append(f\"{path}: References non-existent file {ref}\")\n            results['checks'].append(f\"{path}: Verified\")\n        return results\n",
      "mode": "create_new"
    },
    {
      "id": "llm-documentor-app-patch-01",
      "description": "Replace large in-file engine implementations in app.py with imports from the new engine package to decouple responsibilities (preserve API surface).",
      "op": "patch_text",
      "path": "modular-framework/modules/llm-documentor/app.py",
      "patches": [
        {
          "type": "replace_between",
          "start": "# ============= GitHub Hub Integration =============",
          "end": "# ============= API Endpoints =============",
          "replacement": "# ============= Modular engine imports =============\n# The heavy-lifting implementation (github hub helpers, extractor, normalizer,\n# generator and verifier) has been moved to the engine/ package to keep this\n# file focused on HTTP endpoints and orchestration. The modules retain the\n# original behavior and interfaces.\nfrom engine.github_hub import fetch_repo_tree, fetch_file_content\nfrom engine.extractor import CodeExtractor\nfrom engine.normalizer import ChunkNormalizer\nfrom engine.generator import DocGenerator\nfrom engine.verifier import DocVerifier\n\n# ============= API Endpoints ============="
        }
      ]
    }
  ]
}