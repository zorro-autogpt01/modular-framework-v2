
{
  "version": "1.0",
  "project_root": "",
  "dry_run": false,
  "backup": true,
  "changes": [
    {
      "id": "fix-openai-compat-gpt5-streaming",
      "description": "Workaround: GPT-5 / o5 models sometimes fail when using streaming with the OpenAI-compatible gateway. Disable streaming for GPT-5 to ensure full test runs complete successfully. Implement a guarded non-stream path for GPT-5 while preserving streaming for non-GPT-5 models.",
      "continue_on_error": false,
      "op": "write_file",
      "path": "modular-framework/modules/llm-gateway/server/providers/openaiCompat.js",
      "mode": "overwrite",
      "content": "const axios = require('axios');\nconst { logDebug, logWarn } = require('../logger');\n\nfunction handleResponsesChunk(chunk, onDelta, onDone, onError) {\n  const text = chunk.toString();\n  logDebug('GW RESPONSES stream chunk', { size: Buffer.byteLength(text), head: text.slice(0, 800) });\n  for (const line of text.split('\\n')) {\n    if (!line.startsWith('data:')) continue;\n    const payload = line.replace(/^data:\\s*/, '').trim();\n    if (!payload) continue;\n    if (payload === '[DONE]') { onDone?.(); continue; }\n    try {\n      if (payload === '') continue;\n      const evt = JSON.parse(payload);\n      logDebug('GW RESPONSES stream line parsed', { evtPreview: JSON.stringify(evt).slice(0, 400) });\n      const t = evt?.type || '';\n      if (t === 'response.output_text.delta' || t === 'delta' || t === 'response.output_text') {\n        // best-effort: extract content field if present\n        const delta = evt?.delta ?? evt?.text ?? evt?.output_text?.[0]?.content ?? evt?.content;\n        if (delta) onDelta?.(delta);\n      } else if (evt?.error) {\n        onError?.(evt.error?.message || evt.message || 'Unknown error from Responses stream');\n      } else {\n        onDelta?.(evt?.content || '');\n      }\n      if (evt?.done) onDone?.();\n    } catch (e) {\n      logWarn('GW RESPONSES stream line parse error', { message: e.message, payload: payload.slice(0, 400) });\n    }\n  }\n}\n\nfunction handleChatCompletionsChunk(chunk, onDelta, onDone) {\n  const str = chunk.toString();\n  logDebug('GW CHAT stream chunk', { size: Buffer.byteLength(str), head: str.slice(0, 800) });\n  for (const line of str.split('\\n')) {\n    if (!line.startsWith('data:')) continue;\n    const payload = line.replace(/^data:\\s*/, '').trim();\n    if (!payload) continue;\n    if (payload === '[DONE]') { onDone?.(); continue; }\n    try {\n      const json = JSON.parse(payload);\n      logDebug('GW CHAT stream line parsed', { preview: JSON.stringify(json).slice(0, 400) });\n      const delta = json.choices?.[0]?.delta?.content;\n      if (delta) onDelta?.(delta);\n    } catch (e) {\n      logWarn('GW CHAT stream line parse error', { message: e.message, payload: payload.slice(0, 400) });\n    }\n  }\n}\n\nfunction redactHeaders(h = {}) {\n  const out = { ...(h || {}) };\n  if (out.Authorization) out.Authorization = 'Bearer ***REDACTED***';\n  return out;\n}\n\nasync function callOpenAICompat({\n  baseUrl, apiKey, model, messages, temperature,\n  max_tokens, useResponses, reasoning, stream, onDelta, onDone, onError, rid\n}) {\n  const headers = { 'Content-Type': 'application/json' };\n  if (apiKey) headers['Authorization'] = `Bearer ${apiKey}`;\n\n  const base = baseUrl.replace(/\\/$/, '');\n  // GPT-5 family detection\n  const isGpt5 = /^gpt-5/i.test(model) || /^o5/i.test(model);\n\n  // Disable streaming for GPT-5 to improve reliability across full tests\n  const doStream = !!stream && !isGpt5;\n\n  // RESPONSES path (if supported)\n  if (useResponses) {\n    const url = `${base}/v1/responses`;\n    const body = { model, input: messages, stream: doStream };\n    if (!isGpt5 && typeof temperature === 'number' && !Number.isNaN(temperature)) body.temperature = temperature;\n    if (!isGpt5 && max_tokens) body.max_output_tokens = max_tokens;\n\n    logDebug('GW RESPONSES request', { rid, url, model, stream: doStream, headers: redactHeaders(headers) });\n    logDebug('GW RESPONSES request body', { rid, body });\n\n    if (doStream) {\n      const response = await axios.post(url, body, { headers, responseType: 'stream' });\n      logDebug('GW RESPONSES streaming started', { rid, status: response.status });\n      return new Promise((resolve) => {\n        response.data.on('data', (chunk) => handleResponsesChunk(chunk, onDelta, onDone, onError));\n        response.data.on('end', () => { logDebug('GW RESPONSES stream end', { rid }); onDone?.(); resolve(); });\n        response.data.on('error', (e) => { logWarn('GW RESPONSES stream error', { rid, message: e.message }); onError?.(e.message); resolve(); });\n      });\n    } else {\n      const resp = await axios.post(url, body, { headers });\n      logDebug('GW RESPONSES non-stream response', { rid,\n        status: resp.status,\n        dataHead: JSON.stringify(resp.data)?.slice(0, 1000)\n      });\n      const data = resp.data;\n      const content = data?.output_text?.join?.('') || data?.message?.content || data?.content || '';\n      return { content, raw: data };\n    }\n  }\n\n  // CHAT path (without streaming if GPT-5)\n  const url = `${base}/v1/chat/completions`;\n  const body = { model, messages, stream: doStream };\n  if (typeof temperature === 'number' && !Number.isNaN(temperature)) body.temperature = temperature;\n  if (max_tokens) {\n    if (isGpt5 || reasoning === true) body.max_completion_tokens = max_tokens;\n    else body.max_tokens = max_tokens;\n  }\n\n  logDebug('GW CHAT request', { rid, url, model, stream: doStream, headers: redactHeaders(headers) });\n  logDebug('GW CHAT request body', { rid, body });\n\n  if (doStream) {\n    const response = await axios.post(url, body, { headers, responseType: 'stream' });\n    logDebug('GW CHAT streaming started', { rid, status: response.status });\n    return new Promise((resolve) => {\n      response.data.on('data', (chunk) => handleChatCompletionsChunk(chunk, onDelta, onDone));\n      response.data.on('end', () => { logDebug('GW CHAT stream end', { rid }); onDone?.(); resolve(); });\n      response.data.on('error', (e) => { logWarn('GW CHAT stream error', { rid, message: e.message }); onError?.(e.message); resolve(); });\n    });\n  } else {\n    const resp = await axios.post(url, body, { headers });\n    logDebug('GW CHAT non-stream response', { rid,\n      status: resp.status,\n      dataHead: JSON.stringify(resp.data)?.slice(0, 1000)\n    });\n    const data = resp.data;\n    const content = data?.choices?.[0]?.message?.content || data?.text || data?.content || '';\n    return { content, raw: data };\n  }\n}\n\nmodule.exports = { callOpenAICompat };\n"
    }
  ]
}