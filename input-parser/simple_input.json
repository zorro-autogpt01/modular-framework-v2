{
 "files": [
    {
      "path": "modular-framework/modules/llm-gateway/server/routes/chat.js",
      "content": "const express = require('express');\nconst router = express.Router();\nconst { logInfo, logWarn, logError, logDebug } = require('../logger');\nconst {\n  getModel, getModelByKey, getModelByName, logUsage,\n  getTemplate, getTemplateByName, incrementTemplateUsage,\n  getConversation, addConversationMessage\n} = require('../db');\nconst { callOpenAICompat } = require('../providers/openaiCompat');\nconst { callOllama } = require('../providers/ollama');\nconst {\n  pickEncodingForModel, countTextTokens, countChatTokens\n} = require('../utils/tokens');\nconst { substituteTemplate } = require('./templates');\nconst telemetry = require('../telemetry/interactions');\nconst { SSEManager } = require('../utils/sseManager');\nconst { ProviderRouter } = require('../providers/providerRouter');\n\n// Single provider router instance (used as a hint/fallback)\nconst providerRouter = new ProviderRouter();\n\nfunction isGpt5ModelName(model) { \n  return /^gpt-5/i.test(model) || /^o5/i.test(model); \n}\n\nfunction calcCost({ inTok, outTok, inPerM, outPerM }) {\n  const inc = (Number(inTok) || 0) * (Number(inPerM) || 0) / 1_000_000;\n  const outc = (Number(outTok) || 0) * (Number(outPerM) || 0) / 1_000_000;\n  const total = inc + outc;\n  return total > 0 ? Number(total.toFixed(6)) : null;\n}\n\nasync function resolveModel(body) {\n  if (body.modelId) return await getModel(Number(body.modelId));\n  if (body.modelKey) return await getModelByKey(String(body.modelKey));\n  if (body.model) return await getModelByName(String(body.model));\n  return null;\n}\n\nfunction pickContentFromResponses(data) {\n  if (!data || typeof data !== 'object') return '';\n  if (Array.isArray(data.output)) {\n    const msg = data.output.find(p => p?.type === 'message');\n    const parts = msg?.content;\n    if (Array.isArray(parts)) {\n      const ot = parts.find(p => p?.type === 'output_text' && typeof p?.text === 'string');\n      if (ot?.text) return ot.text;\n      if (typeof parts[0]?.text === 'string') return parts[0].text;\n      if (typeof parts[0]?.content === 'string') return parts[0].content;\n    }\n  }\n  if (typeof data.text === 'string') return data.text;\n  if (typeof data.content === 'string') return data.content;\n  return '';\n}\n\nfunction sanitizeMessages(messages, maxChars = 8000) {\n  const out = [];\n  let remaining = maxChars;\n  for (const m of Array.isArray(messages) ? messages : []) {\n    if (remaining <= 0) break;\n    let text = '';\n    if (typeof m.content === 'string') text = m.content;\n    else if (Array.isArray(m.content)) {\n      text = m.content\n        .filter(p => typeof p?.text === 'string')\n        .map(p => p.text)\n        .join('');\n    }\n    if (text.length > remaining) text = text.slice(0, remaining);\n    remaining -= text.length;\n    out.push({ role: m.role || 'user', content: text });\n  }\n  return out;\n}\n\n// NEW: Apply template if specified\nasync function applyTemplate(body) {\n  const { template_id, template_name, template_variables } = body;\n  \n  if (!template_id && !template_name) return body; // no template\n  \n  let tmpl;\n  if (template_id) {\n    tmpl = await getTemplate(Number(template_id));\n  } else if (template_name) {\n    tmpl = await getTemplateByName(template_name);\n  }\n  \n  if (!tmpl) {\n    throw new Error(`Template not found: ${template_id || template_name}`);\n  }\n  \n  const vars = template_variables || {};\n  const rendered = substituteTemplate(tmpl.template, vars);\n  \n  // Inject as user message (or replace last user message if specified)\n  const messages = body.messages || [];\n  if (body.replace_last_message && messages.length > 0) {\n    messages[messages.length - 1].content = rendered;\n  } else {\n    messages.push({ role: 'user', content: rendered });\n  }\n  \n  return {\n    ...body,\n    messages,\n    _template_id: tmpl.id, // track for usage stats\n    _template_name: tmpl.name\n  };\n}\n\n/**\n * Dispatch upstream call and handle SSE via SSEManager\n * @param {*} modelRow resolved DB model row\n * @param {*} reqBody request body\n * @param {*} res express response (used for non-stream JSON only)\n * @param {*} sse SSEManager instance or null\n * @param {*} rid request id\n * @param {'default'|'workflows'} sseFormat SSE payload format\n */\nasync function dispatch(modelRow, reqBody, res, sse, rid, sseFormat = 'default') {\n  let {\n    messages = [], temperature, max_tokens, stream = true, \n    useResponses, reasoning, metadata, conversation_id,\n    _template_id\n  } = reqBody || {};\n\n  const upstream = {\n    baseUrl: modelRow.provider_base_url,\n    apiKey: modelRow.provider_api_key,\n    model: modelRow.model_name,\n    messages, temperature, max_tokens,\n    useResponses: (modelRow.mode === 'responses') ||\n                  (!!useResponses) ||\n                  (modelRow.supports_responses && isGpt5ModelName(modelRow.model_name)) ||\n                  isGpt5ModelName(modelRow.model_name),\n    reasoning: reasoning || modelRow.supports_reasoning || false,\n    stream\n  };\n\n  // Provider hint (via ProviderRouter) for logging/diagnostics\n  let providerHint = null;\n  try {\n    const p = providerRouter.selectProvider(modelRow.model_name, modelRow.provider_kind);\n    providerHint = p?.name || null;\n  } catch {}\n\n  logInfo('GW LLM request', { rid,\n    provider_kind: modelRow.provider_kind,\n    provider_hint: providerHint,\n    model: upstream.model,\n    stream, useResponses: upstream.useResponses, reasoning: upstream.reasoning,\n    messagesCount: Array.isArray(messages) ? messages.length : 0,\n    conversation_id: conversation_id || null,\n    template_used: _template_id || null\n  });\n\n  const encName = pickEncodingForModel(modelRow.model_name);\n  let promptChars = 0;\n  try { promptChars = JSON.stringify(messages || []).length; } catch {}\n  const inTok = countChatTokens(messages, encName);\n\n  const metaBase = {\n    ...(metadata || {}),\n    gateway: 'llm-gateway',\n    currency: modelRow.currency || 'USD',\n    conversation_id: conversation_id || null,\n    template_id: _template_id || null\n  };\n\n  let completionText = '';\n  let lastPush = 0;\n  const pushLive = () => {\n    const now = Date.now();\n    if (now - lastPush < 150) return;\n    lastPush = now;\n    telemetry.update(rid, { output: completionText });\n  };\n\n  const onDelta = (d) => {\n    logDebug('GW upstream delta', { rid, len: typeof d === 'string' ? d.length : 0 });\n    if (typeof d === 'string' && d) completionText += d;\n    if (sse) {\n      if (sseFormat === 'workflows') sse.send({ type: 'llm.delta', data: d });\n      else sse.sendChunk({ content: d });\n    }\n    pushLive();\n  };\n  // We will complete when usage is computed; upstream onDone acknowledged silently\n  const onDone = () => { logDebug('GW upstream done', { rid }); };\n  const onError = (m) => { \n    logWarn('GW upstream error', { rid, message: m }); \n    if (sse) sse.error(new Error(m));\n  };\n\n  // Helper to finalize usage + SSE completion\n  async function finalizeAndComplete(outTextForAccounting) {\n    telemetry.update(rid, { output: outTextForAccounting });\n    const completionChars = outTextForAccounting.length;\n    const outTok = countTextTokens(outTextForAccounting, encName);\n    const cost = calcCost({\n      inTok, outTok,\n      inPerM: modelRow.input_cost_per_million,\n      outPerM: modelRow.output_cost_per_million\n    });\n    await logUsage({\n      provider_id: modelRow.provider_id,\n      model_id: modelRow.id,\n      conversation_id: conversation_id || metaBase.conversation_id || null,\n      input_tokens: inTok,\n      output_tokens: outTok,\n      prompt_chars: promptChars,\n      completion_chars: completionChars,\n      cost,\n      meta: metaBase\n    });\n\n    if (_template_id) {\n      await incrementTemplateUsage(_template_id, inTok + outTok, cost);\n    }\n    \n    if (conversation_id) {\n      await addConversationMessage({\n        conversation_id,\n        role: 'assistant',\n        content: outTextForAccounting,\n        tokens: outTok,\n        cost\n      });\n    }\n\n    telemetry.finish(rid, {\n      inTok, outTok, cost,\n      model: modelRow.model_name,\n      provider: modelRow.provider_kind,\n      conversation_id: conversation_id || null\n    });\n\n    if (sse) {\n      // Provide usage in completion\n      sse.complete({\n        prompt_tokens: inTok,\n        completion_tokens: outTok,\n        total_tokens: inTok + outTok\n      });\n    }\n  }\n\n  // OLLAMA\n  if (modelRow.provider_kind === 'ollama') {\n    if (stream) {\n      await callOllama({ ...upstream, onDelta, onDone, onError, rid });\n      await finalizeAndComplete(completionText);\n      return;\n    } else {\n      const { content } = await callOllama({ ...upstream, stream:false, rid });\n      const text = content || '';\n      await finalizeAndComplete(text);\n      return res.json({ content: text });\n    }\n  }\n\n  // OpenAI / OpenAI-compatible\n  if (stream) {\n    await callOpenAICompat({ ...upstream, rid, onDelta, onDone, onError });\n    await finalizeAndComplete(completionText);\n  } else {\n    const respPayload = await callOpenAICompat({ ...upstream, rid, stream: false });\n\n    if (upstream.useResponses) {\n      const textForAccounting = pickContentFromResponses(respPayload) || '';\n      await finalizeAndComplete(textForAccounting);\n      return res.json(respPayload);\n    }\n\n    const completionTextLocal = (respPayload && respPayload.content) || '';\n    await finalizeAndComplete(completionTextLocal);\n    return res.json({ content: completionTextLocal });\n  }\n}\n\n// Canonical gateway endpoint\nrouter.post('/v1/chat', async (req, res) => {\n  const rid = req.id;\n  \n  // NEW: Check for dry-run mode\n  if (req.query.dry_run === '1' || req.body?.dry_run === true) {\n    logInfo('GW /api/v1/chat DRY-RUN', { rid, ip: req.ip });\n    \n    try {\n      const body = await applyTemplate(req.body);\n      const modelRow = await resolveModel(body);\n      \n      if (!modelRow) {\n        return res.status(400).json({ \n          error: 'Model not configured',\n          dry_run: true \n        });\n      }\n      \n      const messages = body.messages || [];\n      const encName = pickEncodingForModel(modelRow.model_name);\n      const inTok = countChatTokens(messages, encName);\n      const estOutTok = body.max_tokens || Math.min(inTok * 2, 1000);\n      \n      const cost = calcCost({\n        inTok,\n        outTok: estOutTok,\n        inPerM: modelRow.input_cost_per_million,\n        outPerM: modelRow.output_cost_per_million\n      });\n      \n      return res.json({\n        ok: true,\n        dry_run: true,\n        model: {\n          id: modelRow.id,\n          name: modelRow.model_name,\n          display_name: modelRow.display_name\n        },\n        token_estimate: {\n          input_tokens: inTok,\n          estimated_output_tokens: estOutTok\n        },\n        cost_estimate: {\n          total_cost: cost,\n          currency: modelRow.currency || 'USD'\n        },\n        template_used: body._template_name || null,\n        message_count: messages.length,\n        note: 'Dry-run mode: no LLM call was made'\n      });\n    } catch (err) {\n      return res.status(400).json({ \n        error: err.message,\n        dry_run: true \n      });\n    }\n  }\n\n  const stream = !!(req.body?.stream ?? true);\n  const sse = stream ? new SSEManager(res, rid) : null;\n  if (sse) sse.init();\n\n  logInfo('GW /api/v1/chat <- client', { rid,\n    ip: req.ip,\n    stream,\n    modelId: req.body?.modelId,\n    modelKey: req.body?.modelKey,\n    model: req.body?.model,\n    conversation_id: req.body?.conversation_id,\n    template_id: req.body?.template_id,\n    template_name: req.body?.template_name\n  });\n\n  try {\n    // Apply template if specified\n    const body = await applyTemplate(req.body);\n    \n    telemetry.start({\n      id: rid,\n      model: body?.model || body?.modelKey || body?.modelId || null,\n      provider: null,\n      stream,\n      ip: req.ip,\n      started_at: new Date().toISOString(),\n      meta: {\n        conversation_id: body?.conversation_id || null,\n        template_id: body?._template_id || null\n      }\n    });\n    telemetry.update(rid, {\n      messages: sanitizeMessages(body?.messages)\n    });\n\n    const modelRow = await resolveModel(body);\n    if (!modelRow) {\n      if (sse) sse.error(new Error('Model not configured in gateway.'));\n      telemetry.fail(rid, 'Model not configured');\n      return stream ? undefined : res.status(400).json({ error: 'Model not configured' });\n    }\n    \n    telemetry.update(rid, {\n      model: modelRow.model_name,\n      provider: modelRow.provider_kind\n    });\n    \n    if (stream) await dispatch(modelRow, body, res, sse, rid, 'default');\n    else await dispatch(modelRow, body, res, null, rid, 'default');\n  } catch (err) {\n    logError('GW /v1/chat error', { rid, err: err?.message || String(err) });\n    if (stream) { sse?.error(new Error(err?.message || 'error')); }\n    else res.status(500).json({ error: err?.message || 'error' });\n    telemetry.fail(rid, err?.message || 'error');\n  }\n});\n\n// Compatibility endpoint (accepts llm-chat-like body)\nrouter.post('/compat/llm-chat', async (req, res) => {\n  const rid = req.id;\n  const stream = !!(req.body?.stream ?? true);\n  const sse = stream ? new SSEManager(res, rid) : null;\n  if (sse) sse.init();\n\n  logInfo('GW /api/compat/llm-chat <- client', { rid, ip: req.ip, stream });\n\n  try {\n    const body = await applyTemplate(req.body);\n    \n    telemetry.start({\n      id: rid, model: body?.model || null, provider: null, stream, ip: req.ip,\n      started_at: new Date().toISOString(),\n      meta: { conversation_id: body?.conversation_id || null }\n    });\n    telemetry.update(rid, { messages: sanitizeMessages(body?.messages) });\n\n    const modelRow = await resolveModel(body);\n    if (!modelRow) {\n      if (sse) sse.error(new Error('Model not found in gateway.'));\n      telemetry.fail(rid, 'Model not found');\n      return stream ? undefined : res.status(400).json({ error: 'Model not found' });\n    }\n\n    telemetry.update(rid, { model: modelRow.model_name, provider: modelRow.provider_kind });\n\n    if (stream) await dispatch(modelRow, body, res, sse, rid, 'default');\n    else await dispatch(modelRow, body, res, null, rid, 'default');\n  } catch (err) {\n    logError('GW /compat/llm-chat error', { rid, err: err?.message || String(err) });\n    if (stream) { sse?.error(new Error(err?.message || 'error')); }\n    else res.status(500).json({ error: err?.message || 'error' });\n    telemetry.fail(rid, err?.message || 'error');\n  }\n});\n\n// Workflows-friendly compat endpoint\nrouter.post('/compat/llm-workflows', async (req, res) => {\n  const rid = req.id;\n  const stream = !!(req.body?.stream ?? true);\n  const sse = stream ? new SSEManager(res, rid) : null;\n  if (sse) sse.init();\n\n  logInfo('GW /api/compat/llm-workflows <- workflows', { rid, ip: req.ip, stream });\n\n  try {\n    const body = await applyTemplate(req.body);\n    \n    telemetry.start({\n      id: rid,\n      model: body?.model || body?.modelKey || body?.modelId || null,\n      stream, ip: req.ip,\n      started_at: new Date().toISOString(),\n      meta: { conversation_id: body?.conversation_id || null }\n    });\n    telemetry.update(rid, { messages: sanitizeMessages(body?.messages) });\n\n    const modelRow = await resolveModel(body);\n\n    if (!modelRow) {\n      if (stream) { sse?.error(new Error('Model not found')); }\n      telemetry.fail(rid, 'Model not found');\n      return !stream && res.status(400).json({ error: 'Model not found' });\n    }\n\n    telemetry.update(rid, { model: modelRow.model_name, provider: modelRow.provider_kind });\n\n    if (stream) await dispatch(modelRow, body, res, sse, rid, 'workflows');\n    else await dispatch(modelRow, body, res, null, rid, 'workflows');\n  } catch (err) {\n    logError('GW /compat/llm-workflows error', { rid, err: err?.message || String(err) });\n    if (stream) { sse?.error(new Error(err?.message || 'error')); }\n    else res.status(500).json({ error: err?.message || 'error' });\n    telemetry.fail(rid, err?.message || 'error');\n  }\n});\n\nmodule.exports = { router };\n"
    }
  ]
}