{
  "version": "1.0",
  "_root": "modular-framework",
  "dry_run": false,
  "backup": true,
  "changes": [
    {
      "id": "ldoc-engine-generator-robust-gateway-content-extraction",
      "description": "Robustly extract text from LLM Gateway responses, including OpenAI Responses (GPT-5/o5) payloads returned by the gateway. This prevents empty docs when 'content' is missing or empty by falling back to output/output_text/choices/message shapes.",
      "op": "patch_text",
      "path": "llm-documentor/engine/generator.py",
      "patches": [
        {
          "type": "replace_between",
          "start": "result = await resp.json()",
          "end": "return result.get('content', '# No content generated')",
          "include_anchors": false,
          "replacement": "result = await resp.json(content_type=None)\n                # Robust content extraction to handle GPT-5 Responses payloads or gateway pass-throughs\n                def _pick(resp_obj):\n                    if not isinstance(resp_obj, dict):\n                        return ''\n                    # 1) Direct 'content' field\n                    c = resp_obj.get('content')\n                    if isinstance(c, str) and c.strip():\n                        return c\n                    # 2) Gateway may include 'raw' with upstream payload\n                    raw = resp_obj.get('raw')\n                    candidates = []\n                    if isinstance(raw, (dict, list)):\n                        candidates.append(raw)\n                    candidates.append(resp_obj)\n\n                    for obj in candidates:\n                        if not isinstance(obj, dict):\n                            continue\n                        # 2a) OpenAI Responses style: output_text array\n                        out_text = obj.get('output_text')\n                        if isinstance(out_text, list) and out_text:\n                            joined = ''.join([t for t in out_text if isinstance(t, str)])\n                            if joined.strip():\n                                return joined\n                        # 2b) OpenAI Responses style: output[] -> message -> content[] -> output_text.text\n                        output = obj.get('output')\n                        if isinstance(output, list):\n                            try:\n                                msg = next((p for p in output if isinstance(p, dict) and p.get('type') == 'message'), None)\n                                parts = msg.get('content') if isinstance(msg, dict) else None\n                                if isinstance(parts, list):\n                                    ot = next((p for p in parts if isinstance(p, dict) and p.get('type') == 'output_text' and isinstance(p.get('text'), str)), None)\n                                    if ot and ot.get('text', '').strip():\n                                        return ot['text']\n                                    # fallback to first part text/content\n                                    if parts and isinstance(parts[0], dict):\n                                        if isinstance(parts[0].get('text'), str) and parts[0]['text'].strip():\n                                            return parts[0]['text']\n                                        if isinstance(parts[0].get('content'), str) and parts[0]['content'].strip():\n                                            return parts[0]['content']\n                            except Exception:\n                                pass\n                        # 2c) Legacy chat completions compatible\n                        choices = obj.get('choices')\n                        if isinstance(choices, list) and choices:\n                            m = choices[0].get('message') if isinstance(choices[0], dict) else None\n                            if isinstance(m, dict):\n                                mc = m.get('content')\n                                if isinstance(mc, str) and mc.strip():\n                                    return mc\n                        # 2d) Other fallbacks\n                        if isinstance(obj.get('text'), str) and obj['text'].strip():\n                            return obj['text']\n                        if isinstance(obj.get('content'), str) and obj['content'].strip():\n                            return obj['content']\n                    return ''\n\n                text = _pick(result)\n                return text if text.strip() else '# No content generated'"
        }
      ]
    },
    {
      "id": "ldoc-app-generator-robust-gateway-content-extraction",
      "description": "Same robust extraction fix in the service DocGenerator used by the FastAPI app so both code paths handle GPT-5/o5 Responses payloads without relying on gateway changes.",
      "op": "patch_text",
      "path": "llm-documentor/app.py",
      "patches": [
        {
          "type": "replace_between",
          "start": "result = await resp.json()",
          "end": "return result.get(\"content\", \"# No content generated\")",
          "include_anchors": false,
          "replacement": "result = await resp.json(content_type=None)\n                # Robust content extraction to handle GPT-5 Responses payloads or gateway pass-throughs\n                def _pick(resp_obj):\n                    if not isinstance(resp_obj, dict):\n                        return ''\n                    # 1) Direct 'content'\n                    c = resp_obj.get('content')\n                    if isinstance(c, str) and c.strip():\n                        return c\n                    # 2) Gateway may include 'raw' with upstream payload\n                    raw = resp_obj.get('raw')\n                    candidates = []\n                    if isinstance(raw, (dict, list)):\n                        candidates.append(raw)\n                    candidates.append(resp_obj)\n\n                    for obj in candidates:\n                        if not isinstance(obj, dict):\n                            continue\n                        # Responses: output_text array\n                        out_text = obj.get('output_text')\n                        if isinstance(out_text, list) and out_text:\n                            joined = ''.join([t for t in out_text if isinstance(t, str)])\n                            if joined.strip():\n                                return joined\n                        # Responses: output[] -> message -> content[] -> output_text.text\n                        output = obj.get('output')\n                        if isinstance(output, list):\n                            try:\n                                msg = next((p for p in output if isinstance(p, dict) and p.get('type') == 'message'), None)\n                                parts = msg.get('content') if isinstance(msg, dict) else None\n                                if isinstance(parts, list):\n                                    ot = next((p for p in parts if isinstance(p, dict) and p.get('type') == 'output_text' and isinstance(p.get('text'), str)), None)\n                                    if ot and ot.get('text', '').strip():\n                                        return ot['text']\n                                    if parts and isinstance(parts[0], dict):\n                                        if isinstance(parts[0].get('text'), str) and parts[0]['text'].strip():\n                                            return parts[0]['text']\n                                        if isinstance(parts[0].get('content'), str) and parts[0]['content'].strip():\n                                            return parts[0]['content']\n                            except Exception:\n                                pass\n                        # Legacy chat completions\n                        choices = obj.get('choices')\n                        if isinstance(choices, list) and choices:\n                            m = choices[0].get('message') if isinstance(choices[0], dict) else None\n                            if isinstance(m, dict):\n                                mc = m.get('content')\n                                if isinstance(mc, str) and mc.strip():\n                                    return mc\n                        # Other fallbacks\n                        if isinstance(obj.get('text'), str) and obj['text'].strip():\n                            return obj['text']\n                        if isinstance(obj.get('content'), str) and obj['content'].strip():\n                            return obj['content']\n                    return ''\n\n                text = _pick(result)\n                return text if text.strip() else '# No content generated'"
        }
      ]
    }
  ]
}