version: '3.8'

services:
  # Main Framework Container with Nginx# Edge Nginx (reverse proxy & TLS terminator)
  edge-nginx:
    build: ./edge-nginx
    container_name: edge-nginx
    ports:
      - "8080:80"
      - "8443:443"
    volumes:
      - ./edge-nginx/nginx.conf:/etc/nginx/nginx.conf:ro

    depends_on:
      - framework-web
      - rag-api-module
      - github-hub-module
      - llm-chat
      - llm-workflows-module
      - llm-gateway
      - llm-tester
    networks:
      - app-network
    restart: unless-stopped

  # Framework static website (served behind edge-nginx)
  framework-web:
    build: ./framework
    container_name: framework-web
    expose:
      - "80"
    volumes:
      - ./framework/html:/usr/share/nginx/html
      - ./framework/nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - rag-api-module
      - github-hub-module
      - llm-chat
    networks:
      - app-network
    restart: unless-stopped

  # SSH Terminal Module Container
  ssh-terminal:
    build: ./ssh-terminal
    container_name: ssh-terminal-module
    ports:
      - "3001:3001"
    environment:
      - NODE_ENV=production
      - PORT=3001
    networks:
      - app-network
    restart: unless-stopped
      
  llm-runner-controller:
    build: ./llm-runner-controller
    container_name: llm-runner-controller
    restart: unless-stopped
    environment:
      - RUNNER_TOKEN=${RUNNER_TOKEN}
      - RUNNER_BASE_DIR=/srv/runner
      - INTERNAL_API_TOKEN=${INTERNAL_API_TOKEN:-supersecret}
      - RUNNER_REG_TOKEN=${RUNNER_REG_TOKEN:-supersecret}
      - RUNNER_DEFAULT_TIMEOUT_MS=30000
      - RUNNER_ALLOW_ENV=HTTP_PROXY,HTTPS_PROXY
      - HEALTH_INTERVAL_SEC=15
      - ARTIFACTS_DIR=/app/data/artifacts
    volumes:
      - /srv/runner:/srv/runner
      - runner-controller-data:/app/data
      - runner-artifacts:/app/data/artifacts
    ports:
      - "4010:4010"
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:4015/health"]
      interval: 30s
      timeout: 5s
      retries: 3

  llm-workflows-module:
    build: ./llm-workflows
    container_name: llm-workflows-module
    environment:
      # Logging (optional)
      - SPLUNK_HEC_URL=$SPLUNK_HEC_URL
      - SPLUNK_HEC_TOKEN=$SPLUNK_HEC_TOKEN
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - NODE_ENV=production
      - LLM_RUNNER_CONTROLLER_TOKEN=${LLM_RUNNER_CONTROLLER_TOKEN}
      - LLM_RUNNER_CONTROLLER_BASE=http://llm-runner-controller:4015/api/llm-runner
      - LOG_TO_CONSOLE=true 
      - GITHUB_HUB_BASE=http://github-hub-module:3002
      - DEFAULT_GH_CONN_ID=core-repo
      - REPOOPS_MAX_INPUT_TOKENS=48000
      - REPOOPS_MAX_FILES=20
      - REPOOPS_MAX_FILE_KB=64
      - REPOOPS_MAX_TOTAL_KB=512
      - REPOOPS_TEST_TIMEOUT_MS=900000
      - GITHUB_CLONE_TOKEN=${GITHUB_CLONE_TOKEN}
      # INTERNAL_API_TOKEN:
      #   Protects "dangerous" backend endpoints (testStep with execute, run workflow).
      #   The frontend must send Authorization: Bearer <this token>.
      #   Set it once in the UI via "Set API Token".
      - INTERNAL_API_TOKEN=${INTERNAL_API_TOKEN:-supersecret}
      # RUNNER_REG_TOKEN:
      #   Token that the installer script and agents use to self-register with the workflows server.
      #   Keep this secret; it is accepted at /api/runners/register.
      - RUNNER_REG_TOKEN=${RUNNER_REG_TOKEN:-supersecret}
      # Point to llm-gateway compat endpoint designed for workflows
      - LLM_GATEWAY_CHAT_URL=http://llm-gateway:3010/api/compat/llm-workflows
      - LLM_GATEWAY_API_BASE=http://llm-gateway:3010/api
      # Serve UI under a path (module also supports root)
      #- BASE_PATH=/llm-workflows
      # Allow step execution (local or via runners)
      - ALLOW_STEP_EXEC=true
    ports:
      - "3005:3005"
    volumes:
      - ./llm-workflows/data:/app/data
      - workflows-node-modules:/app/node_modules
    depends_on:
      - llm-gateway
      - github-hub-module
      - llm-runner-controller
      - splunk
    restart: unless-stopped
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3005/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

#  runner-agent:
#    build: ./runner-agent
#    container_name: runner-agent
#    environment:
#      - PORT=4010
#      - AGENT_NAME=lab
#      - AGENT_TOKEN=supersecret
#      # URL that llm-workflows will use to call this agent (inside Docker network use service name)
#      - AGENT_URL=http://runner-agent:4010
#      # Self-register to llm-workflows (optional)
#      - REGISTER_URL=http://llm-workflows-module:3005/api/runners/register
#      - REGISTER_TOKEN=supersecret
#      # Where to execute by default inside the container
#      - DEFAULT_CWD=/workspace
#    volumes:
#      - ./runner-agent:/app:ro
#      - ./llm-workflows/data:/data
#      - ./runner-agent/workspace:/workspace
#    ports:
#      - "4010:4010"
#    depends_on:
#      - llm-workflows-module
#    networks:
#      - app-network
#    restart: unless-stopped


  github-hub-module:
    build:
      context: ./github-hub
    container_name: github-hub-module
    environment:
      - PORT=3002
      - GITHUB_TOKEN=${GITHUB_TOKEN}
      - GH_TOKEN_KEY=${GH_TOKEN_KEY}
      - DATA_DIR=/data
    volumes:
      - ./github-hub/data:/data
    ports:
      - "3002:3002"
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3002/api/health"]
      interval: 30s
      timeout: 5s
      retries: 3

  llm-chat:
    build: ./llm-chat
    container_name: llm-chat-module
    ports:
      - "3004:3004"
    env_file: ./.env
    environment:
      - NODE_ENV=production
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - PORT=3004
      - SPLUNK_HEC_URL=${SPLUNK_HEC_URL}      
      - LOG_TO_CONSOLE=false
      - SPLUNK_HEC_TOKEN=${SPLUNK_HEC_TOKEN}
      - SPLUNK_SOURCE=llm-chat
    depends_on:
      - splunk
    networks:
      - app-network
    restart: unless-stopped

  #browser-module:
  #  build: ./browser
  #  container_name: browser-module
  #  environment:
  #    - PORT=3008
  #    - PUPPETEER_SKIP_CHROMIUM_DOWNLOAD=true
  #    - PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium-browser
  #  expose:
  #    - "3008"
  #  networks:
  #    - app-network
  #  restart: unless-stopped

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - ./RAG/qdrant_storage:/qdrant/storage
    networks:
        - app-network
    environment:
      - QDRANT__LOG_LEVEL=INFO

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    networks:
        - app-network
    volumes:
      - ./RAG/redis_data:/data

  rag-api-module:
    build: ./RAG
    container_name: rag-api-module
    env_file: .env
    ports:
      - "8000:8000"
    environment:
      - QDRANT_URL=http://qdrant:6333
      - REDIS_HOST=redis
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - qdrant
      - redis
    networks:
        - app-network
    volumes:
      - ./RAG/logs:/app/logs

  postgres:
    image: postgres:14-alpine
    container_name: postgres-llm-gateway
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=llm_gateway
    ports:
      - "5432:5432"
    volumes:
      - ./llm-gateway/pgdata:/var/lib/postgresql/data
    networks:
      - app-network
    restart: unless-stopped

  llm-gateway:
    build: ./llm-gateway
    container_name: llm-gateway
    env_file: ./.env
    environment:
      - NODE_ENV=production
      - LOG_TO_CONSOLE=false
      - PORT=3010
      - BASE_PATH=/llm-gateway
      - PGHOST=postgres
      - PGPORT=5432
      - PGUSER=postgres
      - PGPASSWORD=postgres
      - PGDATABASE=llm_gateway
      - SPLUNK_HEC_TOKEN=${SPLUNK_HEC_TOKEN}
      - SPLUNK_HEC_URL=${SPLUNK_HEC_URL}
      - SPLUNK_SOURCE=llm-gateway
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      #- SPLUNK_PASSWORD=${SPLUNK_PASSWORD}
    expose:
      - "3010"
    ports:
      - "3010:3010"
    depends_on:
      - postgres
      - splunk
    networks:
      - app-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3010/health"]
      interval: 30s
      timeout: 5s
      retries: 3

  llm-documentor:
    build: ./llm-documentor
    container_name: llm-documentor
    environment:
      - PORT=3030
      - DATA_DIR=/data
      - GITHUB_HUB_URL=http://github-hub-module:3005/api
      - LLM_GATEWAY_URL=http://llm-gateway:3010/api
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
    volumes:
      - ./llm-documentor/data:/data
      - ./llm-documentor/templates:/app/templates
    ports:
      - "3030:3030"
    depends_on:
      - github-hub-module
      - llm-gateway
    networks:
      - app-network
    restart: unless-stopped

  llm-tester:
    build: ./llm-tester
    container_name: llm-tester
    ports:
      - "3040:3040"
    environment:
      - EDGE_BASE=http://edge-nginx
      - PORT=3040
      - DATA_DIR=/app/data
      - NODE_TLS_REJECT_UNAUTHORIZED=0
      - NODE_EXTRA_CA_CERTS=/etc/ssl/certs/edge-dev.crt
      # Logging to Splunk (optional)
      - SPLUNK_HEC_URL=${SPLUNK_HEC_URL}
      - SPLUNK_HEC_TOKEN=${SPLUNK_HEC_TOKEN}
      - SPLUNK_SOURCE=llm-tester
      - LOG_TO_CONSOLE=false
    volumes:
      - ./llm-tester/data:/app/data
      - ./edge-nginx/certs/dev.crt:/etc/ssl/certs/edge-dev.crt:ro
    networks:
      - app-network
    restart: unless-stopped
    depends_on:
      - splunk
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:3040/api/llm-tester/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 10s
    volumes:
      - ./llm-tester/data:/app/data
      - ./edge-nginx/certs/dev.crt:/etc/ssl/certs/edge-dev.crt:ro
    networks:
      - app-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:3040/api/llm-tester/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
      start_period: 10s


  splunk:
    image: splunk/splunk:latest
    container_name: splunk
    env_file: ./splunk/.env
    environment:
      - SPLUNK_START_ARGS=--accept-license
      - SPLUNK_GENERAL_TERMS=--accept-sgt-current-at-splunk-com
      # If you prefer, you can set SPLUNK_PASSWORD in .env instead of inline
      #- SPLUNK_PASSWORD=$SPLUNK_PASSWORD
      #- SPLUNK_PASSWORD=test
    ports:
      - "7999:8000"  # Web UI
      - "8088:8088"  # HEC
      - "8089:8089"  # mgmt API
    volumes:
      - ./splunk/var:/opt/splunk/var
      - ./splunk/etc:/opt/splunk/etc
      - ./splunk/hec-app:/opt/splunk/etc/apps/hec
    networks:
      - app-network
    restart: unless-stopped
  logging-orchestrator:
    build: ./logging-orchestrator
    container_name: logging-orchestrator
    environment:
      - PORT=3015
      - BASE_PATH=/logging
      # Optional: set a token to require Bearer auth for write endpoints
      - ORCH_TOKEN=${ORCH_TOKEN}
      # Seed known services (you can add more at runtime in the UI)
      - SERVICES=[{"name":"llm-gateway","logging_url":"http://llm-gateway:3010/api/logging"},{"name":"rag","logging_url":"http://rag-api-module:8000/admin-api/logging"},{"name":"llm-chat","logging_url":"http://llm-chat-module:3004/admin-api/logging"},{"name":"llm-tester","logging_url":"http://llm-tester:3040/api/logging"}]
      # Optional: default HEC
      - SPLUNK_HEC_URL=${SPLUNK_HEC_URL}
      - SPLUNK_HEC_TOKEN=${SPLUNK_HEC_TOKEN}
      - SPLUNK_SOURCE=platform
      - SPLUNK_INDEX=${SPLUNK_INDEX}
    volumes:
      - ./logging-orchestrator/data:/data
    ports:
      - "3015:3015"
    networks:
      - app-network
    restart: unless-stopped

  llm-ide-frontend:
    build:
      context: ./llm-ide
      dockerfile: Dockerfile
    container_name: llm-ide-frontend
    ports:
      - "3020:3020"
    depends_on:
      - llm-ide-backend
    networks:
      - app-network

  llm-ide-backend:
    build:
      context: ./llm-ide/backend
      dockerfile: Dockerfile
    container_name: llm-ide-backend
    ports:
      - "3021:3021"
    networks:
      - app-network

volumes:
  workflows-node-modules:
    driver: local
  runner-controller-data:
    driver: local
  runner-artifacts:
    driver: local
  runner-workspace:
    driver: local

networks:
  app-network:
    driver: bridge

    # Production considerations:
# 
# 1. Use external secrets management:
#    environment:
#      - GITHUB_CLONE_TOKEN=${GITHUB_CLONE_TOKEN}
#    
#    Set via:
#    - AWS Secrets Manager
#    - HashiCorp Vault
#    - Kubernetes Secrets
#    - Docker Swarm Secrets
#
# 2. Use named volumes with backup strategy:
#    volumes:
#      runner-controller-data:
#        driver: local
#        driver_opts:
#          type: none
#          device: /mnt/data/runner-controller
#          o: bind
#
# 3. Add resource limits:
#    deploy:
#      resources:
#        limits:
#          cpus: '2'
#          memory: 4G
#        reservations:
#          cpus: '1'
#          memory: 2G
#
# 4. Configure logging:
#    logging:
#      driver: "json-file"
#      options:
#        max-size: "10m"
#        max-file: "3"
#
# 5. Use external network for multi-compose setups:
#    networks:
#      modular-backend:
#        external: true