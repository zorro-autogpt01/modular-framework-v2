version: '3.8'

services:
  # Main Framework Container with Nginx# Edge Nginx (reverse proxy & TLS terminator)
  edge-nginx:
    build: ./edge-nginx
    container_name: edge-nginx
    ports:
      - "8080:80"
      - "8443:443"
    volumes:
      - ./edge-nginx/nginx.conf:/etc/nginx/nginx.conf:ro

    depends_on:
      - framework-web
      - rag-api-module
      - github-hub-module
      - llm-chat
      - llm-workflows-module
      - llm-gateway
    networks:
      - app-network
    restart: unless-stopped

  # Framework static website (served behind edge-nginx)
  framework-web:
    build: ./framework
    container_name: framework-web
    expose:
      - "80"
    volumes:
      - ./framework/html:/usr/share/nginx/html
      - ./framework/nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - rag-api-module
      - github-hub-module
      - llm-chat
    networks:
      - app-network
    restart: unless-stopped

  # SSH Terminal Module Container
  ssh-terminal:
    build: ./ssh-terminal
    container_name: ssh-terminal-module
    ports:
      - "3001:3001"
    environment:
      - NODE_ENV=production
      - PORT=3001
    networks:
      - app-network
    restart: unless-stopped

  llm-workflows-module:
    build: ./llm-workflows
    container_name: llm-workflows-module
    environment:
      # Logging (optional)
      - SPLUNK_HEC_URL=$SPLUNK_HEC_URL
      - SPLUNK_HEC_TOKEN=$SPLUNK_HEC_TOKEN
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - NODE_ENV=production
      # INTERNAL_API_TOKEN:
      #   Protects "dangerous" backend endpoints (testStep with execute, run workflow).
      #   The frontend must send Authorization: Bearer <this token>.
      #   Set it once in the UI via "Set API Token".
      - INTERNAL_API_TOKEN=${INTERNAL_API_TOKEN:-supersecret}
      # RUNNER_REG_TOKEN:
      #   Token that the installer script and agents use to self-register with the workflows server.
      #   Keep this secret; it is accepted at /api/runners/register.
      - RUNNER_REG_TOKEN=${RUNNER_REG_TOKEN:-supersecret}
      # Point to llm-gateway compat endpoint designed for workflows
      - LLM_GATEWAY_CHAT_URL=http://llm-gateway:3010/api/compat/llm-workflows
      - LLM_GATEWAY_API_BASE=http://llm-gateway:3010/api
      # Serve UI under a path (module also supports root)
      - BASE_PATH=/llm-workflows
      # Allow step execution (local or via runners)
      - ALLOW_STEP_EXEC=true
    ports:
      - "3005:3005"
    volumes:
      - ./llm-workflows/data:/app/data
    depends_on:
      - llm-gateway
      - splunk
    restart: unless-stopped
    networks:
      - app-network

  runner-agent:
    build: ./runner-agent
    container_name: runner-agent
    environment:
      - PORT=4010
      - AGENT_NAME=lab
      - AGENT_TOKEN=supersecret
      # URL that llm-workflows will use to call this agent (inside Docker network use service name)
      - AGENT_URL=http://runner-agent:4010
      # Self-register to llm-workflows (optional)
      - REGISTER_URL=http://llm-workflows-module:3005/api/runners/register
      - REGISTER_TOKEN=supersecret
      # Where to execute by default inside the container
      - DEFAULT_CWD=/workspace
    volumes:
      - ./runner-agent:/app:ro
      - ./llm-workflows/data:/data
      - ./runner-agent/workspace:/workspace
    ports:
      - "4010:4010"
    depends_on:
      - llm-workflows-module
    networks:
      - app-network
    restart: unless-stopped


  github-hub-module:
    build:
      context: ./github-hub
    container_name: github-hub-module
    environment:
      - PORT=3005
      - GITHUB_TOKEN=${GITHUB_TOKEN}
      - GH_TOKEN_KEY=${GH_TOKEN_KEY}
      - DATA_DIR=/data
    volumes:
      - ./github-hub/data:/data
    expose:
      - "3005"
    networks:
      - app-network

  llm-chat:
    build: ./llm-chat
    container_name: llm-chat-module
    ports:
      - "3004:3004"
    env_file: ./.env
    environment:
      - NODE_ENV=production
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      - PORT=3004
      - SPLUNK_HEC_URL=${SPLUNK_HEC_URL}      
      - LOG_TO_CONSOLE=false
      - SPLUNK_HEC_TOKEN=${SPLUNK_HEC_TOKEN}
      - SPLUNK_SOURCE=llm-chat
    depends_on:
      - splunk
    networks:
      - app-network
    restart: unless-stopped

  #browser-module:
  #  build: ./browser
  #  container_name: browser-module
  #  environment:
  #    - PORT=3008
  #    - PUPPETEER_SKIP_CHROMIUM_DOWNLOAD=true
  #    - PUPPETEER_EXECUTABLE_PATH=/usr/bin/chromium-browser
  #  expose:
  #    - "3008"
  #  networks:
  #    - app-network
  #  restart: unless-stopped

  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - ./RAG/qdrant_storage:/qdrant/storage
    networks:
        - app-network
    environment:
      - QDRANT__LOG_LEVEL=INFO

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    networks:
        - app-network
    volumes:
      - ./RAG/redis_data:/data

  rag-api-module:
    build: ./RAG
    container_name: rag-api-module
    env_file: .env
    ports:
      - "8000:8000"
    environment:
      - QDRANT_URL=http://qdrant:6333
      - REDIS_HOST=redis
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - qdrant
      - redis
    networks:
        - app-network
    volumes:
      - ./RAG/logs:/app/logs

  postgres:
    image: postgres:14-alpine
    container_name: postgres-llm-gateway
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=llm_gateway
    ports:
      - "5432:5432"
    volumes:
      - ./llm-gateway/pgdata:/var/lib/postgresql/data
    networks:
      - app-network
    restart: unless-stopped

  llm-gateway:
    build: ./llm-gateway
    container_name: llm-gateway
    env_file: ./.env
    environment:
      - NODE_ENV=production
      - LOG_TO_CONSOLE=false
      - PORT=3010
      - BASE_PATH=/llm-gateway
      - PGHOST=postgres
      - PGPORT=5432
      - PGUSER=postgres
      - PGPASSWORD=postgres
      - PGDATABASE=llm_gateway
      - SPLUNK_HEC_TOKEN=${SPLUNK_HEC_TOKEN}
      - SPLUNK_HEC_URL=${SPLUNK_HEC_URL}
      - SPLUNK_SOURCE=llm-gateway
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
      #- SPLUNK_PASSWORD=${SPLUNK_PASSWORD}
    expose:
      - "3010"
    ports:
      - "3010:3010"
    depends_on:
      - postgres
      - splunk
    networks:
      - app-network
    restart: unless-stopped

  llm-documentor:
    build: ./llm-documentor
    container_name: llm-documentor
    environment:
      - PORT=3030
      - DATA_DIR=/data
      - GITHUB_HUB_URL=http://github-hub-module:3005/api
      - LLM_GATEWAY_URL=http://llm-gateway:3010/api
      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}
    volumes:
      - ./llm-documentor/data:/data
      - ./llm-documentor/templates:/app/templates
    ports:
      - "3030:3030"
    depends_on:
      - github-hub-module
      - llm-gateway
    networks:
      - app-network
    restart: unless-stopped

  llm-tester:
    build: ./llm-tester
    container_name: llm-tester
    ports:
      - "3040:3040"
    environment:
      - EDGE_BASE=http://edge-nginx
      - PORT=3040
      - DATA_DIR=/app/data
    volumes:
      - ./llm-tester/data:/app/data
    networks:
      - app-network
    restart: unless-stopped
    depends_on:
      - edge-nginx

  splunk:
    image: splunk/splunk:latest
    container_name: splunk
    env_file: ./splunk/.env
    environment:
      - SPLUNK_START_ARGS=--accept-license
      - SPLUNK_GENERAL_TERMS=--accept-sgt-current-at-splunk-com
      # If you prefer, you can set SPLUNK_PASSWORD in .env instead of inline
      #- SPLUNK_PASSWORD=$SPLUNK_PASSWORD
      #- SPLUNK_PASSWORD=test
    ports:
      - "7999:8000"  # Web UI
      - "8088:8088"  # HEC
      - "8089:8089"  # mgmt API
    volumes:
      - ./splunk/var:/opt/splunk/var
      - ./splunk/etc:/opt/splunk/etc
      - ./splunk/hec-app:/opt/splunk/etc/apps/hec
    networks:
      - app-network
    restart: unless-stopped
  logging-orchestrator:
    build: ./logging-orchestrator
    container_name: logging-orchestrator
    environment:
      - PORT=3015
      - BASE_PATH=/logging
      # Optional: set a token to require Bearer auth for write endpoints
      - ORCH_TOKEN=${ORCH_TOKEN}
      # Seed known services (you can add more at runtime in the UI)
      - SERVICES=[{"name":"llm-gateway","logging_url":"http://llm-gateway:3010/api/logging"},{"name":"rag","logging_url":"http://rag-api-module:8000/admin-api/logging"},{"name":"llm-chat","logging_url":"http://llm-chat-module:3004/admin-api/logging"}]
      # Optional: default HEC
      - SPLUNK_HEC_URL=${SPLUNK_HEC_URL}
      - SPLUNK_HEC_TOKEN=${SPLUNK_HEC_TOKEN}
      - SPLUNK_SOURCE=platform
      - SPLUNK_INDEX=${SPLUNK_INDEX}
    volumes:
      - ./logging-orchestrator/data:/data
    ports:
      - "3015:3015"
    networks:
      - app-network
    restart: unless-stopped

  llm-ide-frontend:
    build:
      context: ./llm-ide
      dockerfile: Dockerfile
    container_name: llm-ide-frontend
    ports:
      - "3020:3020"
    depends_on:
      - llm-ide-backend
    networks:
      - app-network

  llm-ide-backend:
    build:
      context: ./llm-ide/backend
      dockerfile: Dockerfile
    container_name: llm-ide-backend
    ports:
      - "3021:3021"
    networks:
      - app-network


networks:
  app-network:
    driver: bridge