
# modular-framework/modules/llm-workflows/public/css/theme.css
:root { --bg:#1e1e1e; --panel:#252526; --line:#3e3e42; --txt:#d4d4d4; --muted:#969696; --accent:#0e639c; }
*{box-sizing:border-box}
body{margin:0;background:var(--bg);color:var(--txt);font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif}
.header{display:flex;gap:8px;align-items:center;padding:10px 16px;border-bottom:1px solid var(--line);background:var(--panel)}
.grow{flex:1}
button{background:#0e639c;color:#fff;border:none;border-radius:6px;padding:8px 12px;cursor:pointer}
button.ghost{background:#3e3e42}
button.danger{background:#a1260d}
label{font-size:12px;color:var(--muted);display:block;margin:6px 0 4px}
input,select,textarea{width:100%;padding:8px;border:1px solid #3e3e42;background:#1e1e1e;color:#d4d4d4;border-radius:6px}
.row{display:grid;grid-template-columns:1fr 1fr;gap:10px}
.row3{display:grid;grid-template-columns:1fr 1fr 1fr;gap:10px}
.tabs{display:flex;gap:8px;padding:8px 16px;border-bottom:1px solid var(--line);background:var(--panel)}
.tab{background:#252526;border:1px solid #3e3e42;padding:8px 12px;border-radius:8px;cursor:pointer}
.tab.active{background:#0e639c;border-color:#0e639c}
.pane{padding:16px;max-width:1200px;margin:0 auto}
.card{background:#252526;border:1px solid #3e3e42;border-radius:8px;padding:16px;margin:10px 0}
.list{margin-top:8px}
.item{display:grid;grid-template-columns:1fr auto auto;gap:8px;align-items:center;border:1px solid #3e3e42;background:#1e1e1e;border-radius:6px;padding:8px;margin-bottom:8px}
.small{font-size:12px;color:#969696}
.cols{display:grid;grid-template-columns:1fr 1fr;gap:12px}
.pre{white-space:pre-wrap;word-wrap:break-word;background:#181818;border:1px solid #3e3e42;border-radius:6px;padding:8px}
.horz{display:flex;gap:8px;align-items:center}
.badge{background:#111;border:1px solid #3e3e42;border-radius:999px;padding:2px 8px;font-size:11px;color:#ccc}
.success{color:#4ec9b0}
.warn{color:#f4bf75}
.error{color:#f48771}
.mono{font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace}
.kv{display:flex;flex-wrap:wrap;gap:8px}
.kv .kvp{background:#111;border:1px solid #3e3e42;border-radius:6px;padding:4px 8px}
.split{display:grid;grid-template-columns: 440px 1fr; gap: 16px}
@media (max-width: 1100px){ .split{grid-template-columns:1fr} }

# modular-framework/modules/llm-workflows/public/js/sse.js
export function parseStream(onDelta, onEvent, onDone, onError) {
  let buffer = '';
  return (chunk) => {
    buffer += chunk;
    const parts = buffer.split('\n\n');
    buffer = parts.pop();
    for (const part of parts) {
      const line = part.trim();
      if (!line.startsWith('data:')) continue;
      const payload = line.replace(/^data:\s*/, '').trim();
      if (!payload) continue;
      if (payload === '[DONE]') { onDone?.(); continue; }
      try {
        const evt = JSON.parse(payload);
        if (evt.type === 'llm.delta' && typeof evt.data === 'string') {
          onDelta?.(evt.data);
        } else if (evt.type === 'error') {
          onError?.(evt.message || 'error');
        } else if (evt.type === 'done') {
          onDone?.();
        } else {
          onEvent?.(evt);
        }
      } catch {
        // fallback: if chunk is plain delta text
        onDelta?.(payload);
      }
    }
  };
}

# modular-framework/modules/llm-workflows/public/js/api.js
export async function listWorkflows(){
  const r = await fetch('./api/workflows');
  return await r.json();
}
export async function saveWorkflow(wf){
  const r = await fetch('./api/workflows', { method:'POST', headers:{'Content-Type':'application/json'}, body: JSON.stringify(wf) });
  if (!r.ok) throw new Error(await r.text());
  return await r.json();
}
export async function deleteWorkflow(id){
  const r = await fetch(`./api/workflows/${encodeURIComponent(id)}`, { method:'DELETE' });
  if (!r.ok) throw new Error(await r.text());
  return await r.json();
}
export async function getWorkflow(id){
  const r = await fetch(`./api/workflows/${encodeURIComponent(id)}`);
  if (!r.ok) throw new Error(await r.text());
  return await r.json();
}

export async function testWorkflowStream({ id, input, overrides, dryRun, allowExecute }) {
  const r = await fetch(`./api/workflows/${encodeURIComponent(id)}/test/stream`, {
    method:'POST',
    headers:{ 'Content-Type':'application/json' },
    body: JSON.stringify({ input, overrides, dryRun, allowExecute })
  });
  if (!r.ok) throw new Error(await r.text());
  return r.body; // ReadableStream
}

# modular-framework/modules/llm-workflows/public/js/index.page.js
import { listWorkflows, saveWorkflow, deleteWorkflow, getWorkflow, testWorkflowStream } from './api.js';
import { parseStream } from './sse.js';

const dom = {
  list: () => document.getElementById('wfList'),
  id: () => document.getElementById('wfId'),
  name: () => document.getElementById('wfName'),
  description: () => document.getElementById('wfDesc'),
  prompt: () => document.getElementById('wfPrompt'),
  schema: () => document.getElementById('wfSchema'),
  examples: () => document.getElementById('wfExamples'),
  sys: () => document.getElementById('sysPrompt'),
  provider: () => document.getElementById('provider'),
  baseUrl: () => document.getElementById('baseUrl'),
  apiKey: () => document.getElementById('apiKey'),
  model: () => document.getElementById('model'),
  temp: () => document.getElementById('temperature'),
  max: () => document.getElementById('max_tokens'),
  input: () => document.getElementById('testInput'),
  dryRun: () => document.getElementById('dryRun'),
  allowExecute: () => document.getElementById('allowExecute'),
  logs: () => document.getElementById('logs'),
  parsed: () => document.getElementById('parsed'),
  actions: () => document.getElementById('actions'),
  runBtn: () => document.getElementById('runBtn'),
  saveBtn: () => document.getElementById('saveBtn'),
  newBtn: () => document.getElementById('newBtn'),
};

let working = false;

function renderList(items){
  const list = dom.list(); list.innerHTML = '';
  items.forEach(w => {
    const div = document.createElement('div');
    div.className = 'item';
    div.innerHTML = `
      <div>
        <strong>${w.name}</strong>
        <div class="small mono">${w.id}</div>
        <div class="small">${w.description || ''}</div>
      </div>
      <button class="ghost">Edit</button>
      <button class="danger">Delete</button>
    `;
    const [_, editBtn, delBtn] = div.children;
    editBtn.onclick = async () => {
      const data = await getWorkflow(w.id);
      loadForm(data);
    };
    delBtn.onclick = async () => {
      if (!confirm('Delete workflow?')) return;
      await deleteWorkflow(w.id);
      await refreshList();
    };
    list.appendChild(div);
  });
}

async function refreshList(){
  const arr = await listWorkflows();
  renderList(arr);
}

function cleanJsonText(txt) {
  // Strip code fences and trailing explanation
  if (!txt) return '';
  const fence = txt.match(/```(?:json)?\s*([\s\S]*?)\s*```/i);
  if (fence) return fence[1];
  // try to find first { ... } balanced
  const start = txt.indexOf('{');
  const end = txt.lastIndexOf('}');
  if (start >= 0 && end > start) return txt.slice(start, end+1);
  return txt.trim();
}

function loadForm(w){
  dom.id().value = w.id || '';
  dom.name().value = w.name || '';
  dom.description().value = w.description || '';
  dom.prompt().value = w.prompt || '';
  dom.schema().value = JSON.stringify(w.schema || {}, null, 2);
  dom.examples().value = w.examples?.join('\n\n---\n\n') || '';
  dom.sys().value = w.overrides?.system || '';
  dom.provider().value = w.overrides?.provider || 'openai';
  dom.baseUrl().value = w.overrides?.baseUrl || 'https://api.openai.com';
  dom.apiKey().value = w.overrides?.apiKey || '';
  dom.model().value = w.overrides?.model || 'gpt-4o-mini';
  dom.temp().value = w.overrides?.temperature ?? '';
  dom.max().value = w.overrides?.max_tokens ?? '';
}

function collectForm(){
  const examples = dom.examples().value.split('\n\n---\n\n').map(s => s.trim()).filter(Boolean);
  let schemaObj = {};
  try { schemaObj = JSON.parse(dom.schema().value || '{}'); } catch(e) { alert('Schema JSON invalid'); throw e; }
  return {
    id: dom.id().value.trim() || `wf_${Date.now()}_${Math.random().toString(36).slice(2,7)}`,
    name: dom.name().value.trim(),
    description: dom.description().value.trim(),
    prompt: dom.prompt().value,
    schema: schemaObj,
    examples,
    overrides: {
      provider: dom.provider().value,
      baseUrl: dom.baseUrl().value.trim(),
      apiKey: dom.apiKey().value.trim(),
      model: dom.model().value.trim(),
      temperature: dom.temp().value ? Number(dom.temp().value) : undefined,
      max_tokens: dom.max().value ? Number(dom.max().value) : undefined,
      system: dom.sys().value
    }
  };
}

async function onSave(){
  try{
    const wf = collectForm();
    if (!wf.name) { alert('Name is required'); return; }
    await saveWorkflow(wf);
    dom.id().value = wf.id;
    await refreshList();
    alert('Saved.');
  }catch(e){
    alert('Save failed: ' + e.message);
  }
}

async function onRun(){
  if (working) return;
  const id = dom.id().value.trim();
  if (!id) { alert('Save workflow first.'); return; }
  dom.logs().textContent = '';
  dom.parsed().textContent = '';
  dom.actions().innerHTML = '';
  working = true; dom.runBtn().textContent = 'Running…';

  const overrides = collectForm().overrides;
  const body = {
    id,
    input: dom.input().value,
    overrides,
    dryRun: dom.dryRun().checked,
    allowExecute: dom.allowExecute().checked
  };

  try {
    const stream = await testWorkflowStream(body);
    const reader = stream.getReader();
    const decoder = new TextDecoder();
    const accumulate = [];
    const pump = parseStream(
      (d)=> {
        accumulate.push(d);
        dom.logs().textContent += d;
      },
      (evt)=> {
        if (evt.type === 'parse.result') {
          dom.parsed().textContent = JSON.stringify(evt.data, null, 2);
        }
        if (evt.type === 'actions.summary') {
          renderActions(evt.data || []);
        }
        if (evt.type === 'exec.stdout') {
          addActionLog(`stdout: ${evt.data}`, 'success');
        }
        if (evt.type === 'exec.stderr') {
          addActionLog(`stderr: ${evt.data}`, 'warn');
        }
        if (evt.type === 'exec.event') {
          addActionLog(evt.message || JSON.stringify(evt), evt.level || 'small');
        }
        if (evt.type === 'error') {
          addActionLog(`Error: ${evt.message}`, 'error');
        }
      },
      ()=> {
        addActionLog('Done.', 'success');
      },
      (m)=> {
        addActionLog(`Stream error: ${m}`, 'error');
      }
    );
    while (true) {
      const { value, done } = await reader.read();
      if (done) break;
      if (value) pump(decoder.decode(value, { stream:true }));
    }
  } catch (e) {
    alert('Run failed: ' + e.message);
  } finally {
    working = false; dom.runBtn().textContent = 'Run Test';
  }
}

function addActionLog(msg, cls){
  const div = document.createElement('div');
  div.className = `small ${cls || ''}`;
  div.textContent = msg;
  dom.actions().appendChild(div);
}

function renderActions(actions) {
  dom.actions().innerHTML = '';
  if (!actions || !actions.length) {
    dom.actions().innerHTML = '<div class="small">No actions.</div>';
    return;
  }
  actions.forEach((a, idx)=> {
    const el = document.createElement('div');
    el.className = 'card';
    el.innerHTML = `
      <div class="horz">
        <span class="badge">${idx+1}</span>
        <strong>${a.type.toUpperCase()}</strong>
        <span class="small">${a.name || ''}</span>
        <span class="badge">${a.cwd ? 'cwd:'+a.cwd : ''}</span>
      </div>
      <div class="pre mono">${(a.cmd || a.script || a.content || a.path || '')}</div>
    `;
    dom.actions().appendChild(el);
  });
}

function newWorkflow(){
  const blank = {
    id: '',
    name: '',
    description: 'Generate JSON-only actionable steps. Always respond with JSON, no prose.',
    prompt: [
      'You are an automation planner.',
      'Based on the user request, produce a strict JSON object with an "actions" array.',
      'Each action is one of: "bash", "python", "file", "note".',
      'Rules:',
      '- No prose, no markdown code fences. JSON only.',
      '- Use safe, idempotent commands where possible.',
      '- Include descriptive "name".',
      '- For bash: include "cmd" and optional "cwd".',
      '- For python: include "script" and optional "cwd".',
      '- For file: include "path" and "content".',
      'Return fields: { "actions": [...], "summary": "..." }'
    ].join('\n'),
    schema: {
      type: 'object',
      required: ['actions'],
      properties: {
        actions: {
          type: 'array',
          items: {
            type: 'object',
            required: ['type'],
            properties: {
              type: { type: 'string', enum: ['bash', 'python', 'file', 'note'] },
              name: { type: 'string' },
              cwd: { type: 'string' },
              cmd: { type: 'string' },
              script: { type: 'string' },
              path: { type: 'string' },
              content: { type: 'string' }
            },
            additionalProperties: false
          }
        },
        summary: { type: 'string' }
      },
      additionalProperties: true
    },
    examples: [
      JSON.stringify({
        actions: [
          { type:'bash', name:'List current directory', cmd:'ls -la', cwd:'/tmp' },
          { type:'python', name:'Print hello', script:'print("hello")' },
          { type:'file', name:'Create README', path:'README.md', content:'# Project' },
          { type:'note', name:'Reminder', content:'Review outputs manually' }
        ],
        summary: 'Lists files, prints hello, writes a README, and adds a reminder.'
      }, null, 2)
    ],
    overrides: {
      provider: 'openai',
      baseUrl: 'https://api.openai.com',
      apiKey: '',
      model: 'gpt-4o-mini',
      temperature: 0.2,
      max_tokens: 800,
      system: 'You are a strict JSON generator. Output JSON only without markdown.'
    }
  };
  loadForm(blank);
}

document.addEventListener('DOMContentLoaded', async () => {
  await refreshList();
  newWorkflow();

  dom.saveBtn().addEventListener('click', onSave);
  dom.newBtn().addEventListener('click', newWorkflow);
  dom.runBtn().addEventListener('click', onRun);

  // Optional: try reading llm-chat localStorage to prefill
  try {
    const gRaw = localStorage.getItem('llmChatConfig');
    if (gRaw) {
      const g = JSON.parse(gRaw);
      dom.provider().value = g.provider || 'openai';
      dom.baseUrl().value  = g.baseUrl  || 'https://api.openai.com';
      dom.apiKey().value   = g.apiKey   || '';
      dom.model().value    = g.model    || 'gpt-4o-mini';
      dom.temp().value     = g.temperature ?? 0.7;
      dom.max().value      = g.max_tokens ?? '';
    }
  } catch {}
});

# modular-framework/modules/llm-workflows/public/index.html
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>LLM Workflows</title>
  <link rel="stylesheet" href="./css/theme.css"/>
</head>
<body>
  <div class="header">
    <strong>LLM Workflows</strong>
    <div class="grow"></div>
    <span class="small">Build | Validate | Execute</span>
  </div>

  <div class="pane">
    <div class="tabs">
      <div class="tab active">Builder & Test</div>
    </div>

    <div class="split">
      <div>
        <div class="card">
          <div class="horz" style="justify-content:space-between">
            <strong>Workflows</strong>
            <button id="newBtn" class="ghost">New</button>
          </div>
          <div id="wfList" class="list"></div>
        </div>

        <div class="card">
          <strong>Workflow Details</strong>
          <label>ID</label>
          <input id="wfId" placeholder="auto or custom id" />
          
          <label>Name</label>
          <input id="wfName" placeholder="e.g., Bash & Python Planner"/>

          <label>Description</label>
          <textarea id="wfDesc" rows="2" placeholder=""></textarea>

          <label>System Prompt (for LLM)</label>
          <textarea id="sysPrompt" rows="4" placeholder="System prompt"></textarea>

          <div class="row3">
            <div>
              <label>Provider</label>
              <select id="provider">
                <option value="openai">OpenAI</option>
                <option value="openai-compatible">OpenAI-compatible</option>
                <option value="ollama">Ollama</option>
              </select>
            </div>
            <div>
              <label>Model</label>
              <input id="model" placeholder="gpt-4o-mini or llama3"/>
            </div>
            <div>
              <label>Temperature</label>
              <input id="temperature" type="number" step="0.1" min="0" max="2"/>
            </div>
          </div>
          <label>Base URL</label>
          <input id="baseUrl" placeholder="https://api.openai.com or http://ollama:11434" />
          <label>API Key</label>
          <input id="apiKey" placeholder="sk-..." />
          <label>Max tokens</label>
          <input id="max_tokens" type="number" />

          <label>User Prompt Template</label>
          <textarea id="wfPrompt" rows="8" placeholder="Use {{input}} in template. Include explicit JSON-only instructions."></textarea>

          <label>Expected JSON Schema</label>
          <textarea id="wfSchema" rows="12" class="mono"></textarea>

          <label>Few-shot Examples (separated by ---)</label>
          <textarea id="wfExamples" rows="8" class="mono"></textarea>

          <div class="horz" style="margin-top:8px">
            <button id="saveBtn">Save</button>
          </div>
        </div>
      </div>

      <div>
        <div class="card">
          <strong>Tester</strong>
          <label>Input</label>
          <textarea id="testInput" rows="5" placeholder="Describe what you want the workflow to plan"></textarea>
          <div class="horz">
            <label><input type="checkbox" id="dryRun" checked /> Dry-run (no side-effects)</label>
            <label><input type="checkbox" id="allowExecute" /> Allow execution (DANGEROUS)</label>
          </div>
          <div class="horz" style="margin-top:8px">
            <button id="runBtn">Run Test</button>
          </div>
        </div>

        <div class="card">
          <strong>LLM Stream</strong>
          <div id="logs" class="pre mono" style="min-height:130px"></div>
        </div>

        <div class="card">
          <strong>Parsed JSON</strong>
          <div id="parsed" class="pre mono" style="min-height:130px"></div>
        </div>

        <div class="card">
          <strong>Actions / Execution Log</strong>
          <div id="actions" class="list"></div>
        </div>
      </div>
    </div>
  </div>

  <script type="module" src="./js/index.page.js"></script>
</body>
</html>

# modular-framework/modules/llm-workflows/server/logger.js
const LOG_LEVEL = (process.env.LOG_LEVEL || 'info').toLowerCase();
const LOG_MAX = Number(process.env.LOG_MAX || 1000);
const logs = [];

function add(level, msg, meta) {
  const entry = { ts: new Date().toISOString(), level, msg, ...(meta||{}) };
  logs.push(entry); if (logs.length > LOG_MAX) logs.shift();
  const line = `[${entry.ts}] [${level.toUpperCase()}] ${msg} ${meta?JSON.stringify(meta):''}`;
  if (level === 'debug' && LOG_LEVEL === 'debug') console.debug(line);
  else if (level === 'info' && ['debug','info'].includes(LOG_LEVEL)) console.info(line);
  else if (level === 'warn' && LOG_LEVEL !== 'error') console.warn(line);
  else if (level === 'error') console.error(line);
}
const logDebug = (m,meta)=>add('debug',m,meta);
const logInfo = (m,meta)=>add('info',m,meta);
const logWarn = (m,meta)=>add('warn',m,meta);
const logError = (m,meta)=>add('error',m,meta);
module.exports = { logs, logDebug, logInfo, logWarn, logError };

# modular-framework/modules/llm-workflows/server/storage.js
const fs = require('fs');
const path = require('path');

const DATA_DIR = process.env.DATA_DIR || path.join(__dirname, '..', 'data');
const FILE = path.join(DATA_DIR, 'workflows.json');

function ensureDir() { if (!fs.existsSync(DATA_DIR)) fs.mkdirSync(DATA_DIR, { recursive: true }); }

function loadAll() {
  ensureDir();
  if (!fs.existsSync(FILE)) return [];
  try { return JSON.parse(fs.readFileSync(FILE, 'utf8')); } catch { return []; }
}
function saveAll(arr) {
  ensureDir();
  fs.writeFileSync(FILE, JSON.stringify(arr, null, 2));
}

function upsert(wf) {
  const arr = loadAll();
  const idx = arr.findIndex(x => x.id === wf.id);
  if (idx >= 0) arr[idx] = wf; else arr.push(wf);
  saveAll(arr); return wf;
}
function remove(id) {
  const arr = loadAll().filter(x => x.id !== id);
  saveAll(arr);
}
function get(id) {
  return loadAll().find(x => x.id === id);
}

module.exports = { loadAll, saveAll, upsert, remove, get };

# modular-framework/modules/llm-workflows/server/llmClient.js
const axios = require('axios');
const { logDebug, logWarn } = require('./logger');

// Streams via SSE and forwards deltas to callbacks. Uses llm-chat /api/chat.
async function chatStream({ llmChatUrl, overrides, messages, onDelta, onError, onDone }) {
  const url = (llmChatUrl || process.env.LLM_CHAT_URL || 'http://localhost:3004/api/chat').replace(/\/$/, '');
  const {
    provider='openai', baseUrl, apiKey, model,
    temperature, max_tokens, system
  } = overrides || {};

  const payload = {
    provider, baseUrl, apiKey, model,
    messages: system ? [{ role:'system', content: system }, ...messages] : messages,
    temperature, max_tokens, stream: true
  };

  logDebug('Calling llm-chat', { url, provider, model, baseUrl });

  const resp = await axios.post(url, payload, { responseType: 'stream' });
  resp.data.on('data', (chunk) => {
    const str = chunk.toString();
    for (const line of str.split('\n')) {
      if (!line.startsWith('data:')) continue;
      const payload = line.replace(/^data:\s*/, '').trim();
      if (!payload) continue;
      if (payload === '[DONE]') { onDone?.(); continue; }
      try {
        const evt = JSON.parse(payload);
        if (evt.type === 'delta' && evt.content) {
          onDelta?.(String(evt.content));
        } else if (evt.type === 'error') {
          onError?.(evt.message || 'LLM error');
        } else if (evt.type === 'done') {
          onDone?.();
        } else {
          const content =
            evt?.choices?.[0]?.delta?.content ??
            evt?.output_text?.[0]?.content ??
            evt?.message?.content ??
            evt?.content;
          if (content) onDelta?.(String(content));
        }
      } catch (e) {
        // ignore malformed keepalives
      }
    }
  });
  resp.data.on('end', () => onDone?.());
  resp.data.on('error', (e) => { logWarn('llm-chat stream error', { msg:e.message }); onError?.(e.message); });
}

module.exports = { chatStream };

# modular-framework/modules/llm-workflows/server/executor.js
const { spawn } = require('child_process');
const fs = require('fs');
const path = require('path');

function sanitizeCwd(cwd) {
  if (!cwd) return undefined;
  // prevent weird characters and traversal outside sandbox assigned
  return cwd.replace(/\0/g,'').trim();
}

function execBash({ cmd, cwd, env, timeoutMs=20000 }, onStdout, onStderr) {
  return new Promise((resolve) => {
    const child = spawn('bash', ['-lc', cmd], { cwd: cwd || undefined, env: { ...process.env, ...(env||{}) } });
    let killed = false;
    const timer = setTimeout(()=> { killed = true; child.kill('SIGKILL'); }, timeoutMs);
    child.stdout.on('data', d => onStdout?.(d.toString()));
    child.stderr.on('data', d => onStderr?.(d.toString()));
    child.on('close', (code, signal) => {
      clearTimeout(timer);
      resolve({ code, signal, killed });
    });
  });
}

function execPython({ script, cwd, env, timeoutMs=20000 }, onStdout, onStderr) {
  return new Promise((resolve) => {
    const child = spawn('python3', ['-c', script], { cwd: cwd || undefined, env: { ...process.env, ...(env||{}) } });
    let killed = false;
    const timer = setTimeout(()=> { killed = true; child.kill('SIGKILL'); }, timeoutMs);
    child.stdout.on('data', d => onStdout?.(d.toString()));
    child.stderr.on('data', d => onStderr?.(d.toString()));
    child.on('close', (code, signal) => {
      clearTimeout(timer);
      resolve({ code, signal, killed });
    });
  });
}

function writeFileSafe({ baseDir, filePath, content }) {
  const rel = filePath.replace(/^~\//, '');
  const safePath = path.normalize(path.join(baseDir, rel));
  if (!safePath.startsWith(baseDir)) {
    throw new Error('Path traversal not allowed');
  }
  fs.mkdirSync(path.dirname(safePath), { recursive: true });
  fs.writeFileSync(safePath, content);
  return safePath;
}

module.exports = { execBash, execPython, writeFileSafe, sanitizeCwd };

# modular-framework/modules/llm-workflows/server/app.js
const express = require('express');
const cors = require('cors');
const path = require('path');
const bodyParser = require('body-parser');
const Ajv = require('ajv').default;
const { logs, logInfo, logWarn, logError } = require('./logger');
const { loadAll, upsert, remove, get } = require('./storage');
const { chatStream } = require('./llmClient');
const { execBash, execPython, writeFileSafe, sanitizeCwd } = require('./executor');
const fs = require('fs');
const os = require('os');

const app = express();
const PORT = process.env.PORT || 3005;
const BASE_PATH = (process.env.BASE_PATH || '').replace(/\/$/, '');

app.use(cors({ origin: true, credentials: true }));
app.use(bodyParser.json({ limit: '2mb' }));

const pub = path.join(__dirname, '..', 'public');
app.use(express.static(pub));
if (BASE_PATH) app.use(BASE_PATH, express.static(pub));

app.get('/', (_req, res) => res.sendFile(path.join(pub, 'index.html')));
if (BASE_PATH) app.get(`${BASE_PATH}/`, (_req, res) => res.sendFile(path.join(pub, 'index.html')));

app.get('/health', (_req, res) => res.json({ status: 'healthy' }));
app.get('/api/info', (_req, res) => res.json({ module:'llm-workflows', version:'0.1.0', status:'ready' }));
app.get('/api/logs', (req, res) => {
  const limit = Math.max(1, Math.min(Number(req.query.limit || 200), 2000));
  const start = Math.max(0, logs.length - limit);
  res.json(logs.slice(start));
});

// Workflows CRUD
app.get('/api/workflows', (_req, res) => res.json(loadAll()));
app.get('/api/workflows/:id', (req, res) => {
  const w = get(req.params.id);
  if (!w) return res.status(404).json({ error: 'not found' });
  res.json(w);
});
app.post('/api/workflows', (req, res) => {
  const wf = req.body || {};
  if (!wf.id || !wf.name) return res.status(400).json({ error: 'id and name required' });
  upsert(wf);
  res.json({ ok:true, id: wf.id });
});
app.delete('/api/workflows/:id', (req, res) => {
  remove(req.params.id);
  res.json({ ok:true });
});

// Helpers
function toSSE(res) {
  res.setHeader('Content-Type', 'text/event-stream');
  res.setHeader('Cache-Control', 'no-cache, no-transform');
  res.setHeader('Connection', 'keep-alive');
  res.flushHeaders?.();
  const send = (obj)=> { try { res.write(`data: ${JSON.stringify(obj)}\n\n`); } catch { try { res.end(); } catch {} } };
  return send;
}
function jsonFromText(text){
  if (!text) return null;
  // strip fences
  const fence = text.match(/```(?:json)?\s*([\s\S]*?)\s*```/i);
  const raw = fence ? fence[1] : text;
  const start = raw.indexOf('{'); const end = raw.lastIndexOf('}');
  const slice = (start>=0 && end>start) ? raw.slice(start, end+1) : raw;
  try { return JSON.parse(slice); } catch { return null; }
}

function buildUserPrompt(wf, input) {
  const t = wf.prompt || '';
  return t.replace(/\{\{\s*input\s*\}\}/g, input ?? '');
}

// Streamed Tester
app.post('/api/workflows/:id/test/stream', async (req, res) => {
  const { id } = req.params;
  const { input, overrides, dryRun=true, allowExecute=false } = req.body || {};
  const wf = get(id);
  if (!wf) return res.status(404).json({ error: 'workflow not found' });

  const send = toSSE(res);

  try {
    // Build messages: system + examples + user
    const messages = [];
    if (wf.examples?.length) {
      messages.push({ role:'system', content:'Follow examples strictly. Format output exactly as shown.' });
      for (const ex of wf.examples) {
        messages.push({ role:'user', content:'EXAMPLE_INPUT' });
        messages.push({ role:'assistant', content: ex });
      }
    }

    const userContent = buildUserPrompt(wf, input);
    messages.push({ role:'user', content: userContent });

    let full = '';
    await chatStream({
      overrides: { ...(wf.overrides||{}), ...(overrides||{}) },
      messages,
      onDelta: (d) => { full += d; send({ type:'llm.delta', data:d }); },
      onError: (m) => { send({ type:'error', message:m }); },
      onDone: () => { send({ type:'llm.done' }); }
    });

    // Parse
    const parsed = jsonFromText(full);
    if (!parsed) {
      send({ type:'error', message:'Failed to parse JSON from LLM output.' });
      return res.end();
    }

    // Validate
    const ajv = new Ajv({ allErrors: true, strict:false });
    const validate = ajv.compile(wf.schema || {});
    const valid = validate(parsed);
    if (!valid) {
      send({ type:'error', message:'Schema validation failed', details: validate.errors });
      send({ type:'parse.result', data: parsed });
      return res.end();
    }
    send({ type:'parse.result', data: parsed });

    // Extract actions
    const actions = Array.isArray(parsed.actions) ? parsed.actions : [];
    send({ type:'actions.summary', data: actions });

    // Execution sandbox
    const sandboxRoot = fs.mkdtempSync(path.join(os.tmpdir(), 'wf-'));
    const execAllowed = allowExecute && !dryRun && process.env.ALLOW_DANGEROUS === 'true';

    // Execute actions
    for (const a of actions) {
      const type = String(a.type || '').toLowerCase();
      const cwd = sanitizeCwd(a.cwd) ? path.resolve(sandboxRoot, a.cwd) : sandboxRoot;
      if (!fs.existsSync(cwd)) fs.mkdirSync(cwd, { recursive: true });

      if (!execAllowed) {
        send({ type:'exec.event', level:'small', message:`DRY RUN: would execute ${type} ${a.name || ''}` });
        continue;
      }

      if (type === 'bash' && a.cmd) {
        send({ type:'exec.event', level:'small', message:`bash: ${a.cmd}` });
        // eslint-disable-next-line no-await-in-loop
        const resExec = await execBash({ cmd:a.cmd, cwd }, (s)=> send({ type:'exec.stdout', data:s }), (e)=> send({ type:'exec.stderr', data:e }));
        send({ type:'exec.event', level:'small', message:`exit ${resExec.code}${resExec.killed?' (killed)':''}` });
      } else if (type === 'python' && a.script) {
        send({ type:'exec.event', level:'small', message:`python: ${a.name || ''}` });
        // eslint-disable-next-line no-await-in-loop
        const resExec = await execPython({ script:a.script, cwd }, (s)=> send({ type:'exec.stdout', data:s }), (e)=> send({ type:'exec.stderr', data:e }));
        send({ type:'exec.event', level:'small', message:`exit ${resExec.code}${resExec.killed?' (killed)':''}` });
      } else if (type === 'file' && a.path) {
        const safe = writeFileSafe({ baseDir:sandboxRoot, filePath:a.path, content:String(a.content || '') });
        send({ type:'exec.event', level:'small', message:`file written: ${safe}` });
      } else if (type === 'note') {
        send({ type:'exec.event', level:'small', message:`note: ${a.content || a.name || ''}` });
      } else {
        send({ type:'exec.event', level:'warn', message:`Unknown or incomplete action: ${type}` });
      }
    }

    send({ type:'done' });
    res.end();
  } catch (e) {
    logError('run error', { err: e.message });
    try { send({ type:'error', message: e.message }); } catch {}
    try { res.end(); } catch {}
  }
});

module.exports = app;

# modular-framework/modules/llm-workflows/server/index.js
const app = require('./app');
const PORT = process.env.PORT || 3005;
app.listen(PORT, () => {
  console.log(`LLM Workflows listening on :${PORT}`);
});

# modular-framework/modules/llm-workflows/package.json
{
  "name": "llm-workflows",
  "version": "0.1.0",
  "description": "Workflow builder for LLM JSON actions with test/execute harness",
  "main": "server/index.js",
  "scripts": {
    "start": "node server/index.js",
    "dev": "nodemon server/index.js"
  },
  "dependencies": {
    "ajv": "^8.17.1",
    "axios": "^1.7.2",
    "body-parser": "^1.20.2",
    "cors": "^2.8.5",
    "express": "^4.19.2"
  },
  "devDependencies": {
    "nodemon": "^3.1.0"
  }
}

# modular-framework/modules/llm-workflows/Dockerfile
FROM node:18-alpine
WORKDIR /app

COPY package.json ./
RUN npm install --production

COPY . .

ENV NODE_ENV=production
ENV PORT=3005

EXPOSE 3005

HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD wget -qO- http://localhost:$PORT/health || exit 1

CMD ["node", "server/index.js"]

# modular-framework/modules/llm-workflows/README.txt
LLM Workflows module

- Start server:
  npm start
  Defaults to port 3005. UI at http://localhost:3005

- It expects llm-chat to be running at http://localhost:3004 (default) unless LLM_CHAT_URL is set.
  You do not need to provide model credentials here if your workflow overrides include those.

- Execution is disabled by default. To allow actual execution on the host, set:
  export ALLOW_DANGEROUS=true
  and uncheck "Dry-run" and check "Allow execution" in the tester.
  All execution happens in a temp sandbox directory under /tmp.

- Workflow JSON schema defines the expected output. Default schema supports actions:
  - bash: { type, name?, cmd, cwd? }
  - python: { type, name?, script, cwd? }
  - file: { type, name?, path, content }
  - note: { type, name?, content }

Notes and usage:
- Use {{input}} placeholder in the prompt template.
- Provide examples as pure JSON blocks (no backticks). Separate multiple examples with a line of --- between blocks.
- The tester streams:
  - llm.delta for token deltas
  - parse.result when JSON is parsed
  - actions.summary listing all actions
  - exec.stdout/exec.stderr during execution (if enabled)
  - exec.event for other events
  - error on failures

Security:
- Even with ALLOW_DANGEROUS=true, this runs on your machine. Use only on isolated test environments.


How it works
- Builder UI lets you craft a strict JSON-only prompt, schema, and examples.
- On Run Test, the backend streams from llm-chat, assembles full output, parses and validates JSON using AJV, then executes if allowed.
- Actions run in an ephemeral directory via bash/python, or file writes are constrained to this sandbox.

Integration with llm-chat
- The module posts to llm-chat /api/chat, sending provider/baseUrl/apiKey/model from the workflow overrides. You can copy the values from llm-chat’s settings. The UI tries to prefill from llm-chat’s localStorage if on the same origin.

————————

You can now:
- Start llm-chat (default port 3004)
- Start llm-workflows (default port 3005)
- Open http://localhost:3005, create a workflow, test it, and capture JSON-defined commands/scripts/files.