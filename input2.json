{
  "version": "1.0",
  "backup": true,
  "changes": [
    {
      "id": "llm-chat-logger-update",
      "description": "Route all logs to Splunk, fix module resolution, add structured logging and disable console unless enabled",
      "op": "write_file",
      "path": "modular-framework/modules/llm-chat/server/logger.js",
      "mode": "overwrite",
      "content": "const path = require('path');\nconst LOG_LEVEL = (process.env.LOG_LEVEL || 'info').toLowerCase();\nconst LOG_MAX = Number(process.env.LOG_MAX || 1000);\nconst LOG_TO_CONSOLE = (process.env.LOG_TO_CONSOLE || 'false').toLowerCase() === 'true';\nconst IS_SPLUNK_CONFIGURED = Boolean(process.env.SPLUNK_HEC_URL && process.env.SPLUNK_HEC_TOKEN);\n\n// Try multiple possible locations for the splunk-logger helper\nlet SPLUNK_LOGGER = null;\n(function resolveSplunkLogger(){\n  const candidates = [\n    '/splunk-logger',\n    path.join(__dirname, '..', 'splunk-logger'),\n    path.join(__dirname, '..', '..', 'splunk-logger'),\n    path.join(__dirname, '..', '..', '..', 'splunk-logger')\n  ];\n  for (const modPath of candidates) {\n    try {\n      // eslint-disable-next-line import/no-dynamic-require, global-require\n      SPLUNK_LOGGER = require(modPath);\n      break;\n    } catch (e) {\n      // continue\n    }\n  }\n})();\n\nconst logs = [];\nlet reqCounter = 0;\n\nfunction redact(obj) {\n  if (!obj || typeof obj !== 'object') return obj;\n  const clone = JSON.parse(JSON.stringify(obj));\n  if (clone.apiKey) clone.apiKey = '***REDACTED***';\n  if (clone.headers && clone.headers.Authorization) clone.headers.Authorization = '***REDACTED***';\n  if (clone.headers && clone.headers.authorization) clone.headers.authorization = '***REDACTED***';\n  return clone;\n}\nfunction safeStringify(v) {\n  try {\n    const seen = new WeakSet();\n    return JSON.stringify(v, (k, val) => {\n      if (typeof val === 'object' && val !== null) {\n        if (seen.has(val)) return '[Circular]';\n        seen.add(val);\n      }\n      return val;\n    });\n  } catch {\n    return '[unstringifiable]';\n  }\n}\n\nfunction shouldConsole(level){\n  if (LOG_TO_CONSOLE) return true;\n  // If Splunk isn't configured, still emit to console so we don't go dark\n  return !IS_SPLUNK_CONFIGURED;\n}\nfunction consoleOut(level, line){\n  try {\n    if (!shouldConsole(level)) return;\n    if (level === 'debug' && LOG_LEVEL === 'debug') console.debug(line);\n    else if (level === 'info' && (LOG_LEVEL === 'debug' || LOG_LEVEL === 'info')) console.info(line);\n    else if (level === 'warn' && (LOG_LEVEL !== 'error')) console.warn(line);\n    else if (level === 'error') console.error(line);\n  } catch {}\n}\n\nfunction addLog(level, msg, meta) {\n  const entry = { ts: new Date().toISOString(), level, msg, ...meta };\n  logs.push(entry);\n  if (logs.length > LOG_MAX) logs.shift();\n  const line = `[${entry.ts}] [${level.toUpperCase()}] ${msg} ${meta ? safeStringify(meta) : ''}`;\n  consoleOut(level, line);\n}\n\nfunction augmentMeta(meta){\n  const base = meta && typeof meta === 'object' ? meta : {};\n  return { service: 'llm-chat', ...base };\n}\n\nconst logDebug = (msg, meta)=> {\n  const m = augmentMeta(meta);\n  addLog('debug', msg, m);\n  try { SPLUNK_LOGGER?.logDebug?.(msg, m); } catch {}\n};\nconst logInfo  = (msg, meta)=> {\n  const m = augmentMeta(meta);\n  addLog('info', msg, m);\n  try { SPLUNK_LOGGER?.logInfo?.(msg, m); } catch {}\n};\nconst logWarn  = (msg, meta)=> {\n  const m = augmentMeta(meta);\n  addLog('warn', msg, m);\n  try { SPLUNK_LOGGER?.logWarn?.(msg, m); } catch {}\n};\nconst logError = (msg, meta)=> {\n  const m = augmentMeta(meta);\n  addLog('error', msg, m);\n  try { SPLUNK_LOGGER?.logError?.(msg, m); } catch {}\n};\n\nfunction stamp(req, _res, next) {\n  req.id = `${Date.now().toString(36)}-${(++reqCounter).toString(36)}`;\n  next();\n}\n\nmodule.exports = { logs, redact, safeStringify, addLog, logDebug, logInfo, logWarn, logError, stamp };\n"
    },
    {
      "id": "llm-chat-app-add-http-logging",
      "description": "Add request ID stamping and HTTP access logging to app.js",
      "op": "write_file",
      "path": "modular-framework/modules/llm-chat/server/app.js",
      "mode": "overwrite",
      "content": "const express = require('express');\nconst cors = require('cors');\nconst path = require('path');\nconst bodyParser = require('body-parser');\n\nconst { router: logsRouter } = require('./routes/logs');\nconst { router: infoRouter } = require('./routes/info');\nconst { router: healthRouter } = require('./routes/health');\nconst { router: chatRouter } = require('./routes/chat');\nconst { router: workflowsRouter } = require('./routes/workflows');\nconst { router: agentRouter } = require('./routes/agent');\nconst { stamp, logInfo } = require('./logger');\n\nconst app = express();\n\nconst BASE_PATH = (process.env.BASE_PATH || '').replace(/\\/$/, ''); // e.g. \"/modules/llm-chat\" or \"\"\n\n// middleware\napp.use(cors({ origin: true, credentials: true }));\napp.use(bodyParser.json({ limit: '2mb' }));\napp.use(stamp);\n\n// lightweight HTTP access logging (to Splunk)\napp.use((req, res, next) => {\n  const start = process.hrtime.bigint();\n  res.on('finish', () => {\n    const durMs = Number(process.hrtime.bigint() - start) / 1e6;\n    logInfo('http_access', {\n      rid: req.id,\n      method: req.method,\n      path: req.originalUrl || req.url,\n      status: res.statusCode,\n      duration_ms: Math.round(durMs),\n      ip: req.headers['x-forwarded-for'] || req.socket?.remoteAddress || 'unknown',\n      ua: req.headers['user-agent'] || ''\n    });\n  });\n  next();\n});\n\n// static UI\nconst pub = path.join(__dirname, '..', 'public');\n\n// Serve both at root and at BASE_PATH to support either proxy style\napp.use(express.static(pub));\nif (BASE_PATH) app.use(BASE_PATH, express.static(pub));\n\napp.get('/', (_req, res) => res.sendFile(path.join(pub, 'index.html')));\nif (BASE_PATH) app.get(`${BASE_PATH}/`, (_req, res) => res.sendFile(path.join(pub, 'index.html')));\n\napp.get('/config', (_req, res) => res.sendFile(path.join(pub, 'config.html')));\nif (BASE_PATH) app.get(`${BASE_PATH}/config`, (_req, res) => res.sendFile(path.join(pub, 'config.html')));\n\n// basic routes\napp.use('/', healthRouter);            // /health (root for Docker healthcheck)\napp.use('/api', infoRouter);           // /api/info\napp.use('/api', logsRouter);           // /api/logs, /api/logs/clear\napp.use('/api', workflowsRouter);\napp.use('/api', agentRouter);\nif (BASE_PATH) {\n  app.use(`${BASE_PATH}/api`, workflowsRouter);\n  app.use(`${BASE_PATH}/api`, agentRouter);\n}\nif (BASE_PATH) {\n  app.use(`${BASE_PATH}/api`, infoRouter);  // /modules/llm-chat/api/info\n  app.use(`${BASE_PATH}/api`, logsRouter);  // /modules/llm-chat/api/logs\n}\n\n// chat routes (root and prefixed)\napp.use('/api', chatRouter); // /api/chat\nif (BASE_PATH) app.use(`${BASE_PATH}/api`, chatRouter); // /modules/llm-chat/api/chat\n\n// central error handler (ensures errors are logged and returned as JSON)\napp.use((err, _req, res, _next) => {\n  try {\n    const { logError } = require('./logger');\n    logError('unhandled_error', { message: err?.message || String(err), stack: err?.stack });\n  } catch {}\n  res.status(500).json({ error: 'Internal Server Error' });\n});\n\nmodule.exports = app;\n"
    },
    {
      "id": "llm-chat-index-listen-log",
      "description": "Use Splunk logger for startup message only (no console.log)",
      "op": "write_file",
      "path": "modular-framework/modules/llm-chat/server/index.js",
      "mode": "overwrite",
      "content": "const app = require('./app');\nconst { logInfo } = require('./logger');\nconst LOG_LEVEL = (process.env.LOG_LEVEL || 'info').toLowerCase();\n\nconst PORT = process.env.PORT || 3004;\napp.listen(PORT, () => {\n  logInfo('llm-chat:listen', { port: Number(PORT), log_level: LOG_LEVEL, ts: new Date().toISOString() });\n});\n"
    },
    {
      "id": "llm-chat-routes-chat-augment-logging",
      "description": "Enrich chat route logs with conversationId, message counts and robust stream lifecycle logs",
      "op": "write_file",
      "path": "modular-framework/modules/llm-chat/server/routes/chat.js",
      "mode": "overwrite",
      "content": "const express = require('express');\nconst router = express.Router();\nconst { logInfo, logWarn, logError, logDebug, redact } = require('../logger');\nconst { extractErrAsync } = require('../util/http');\nconst { handleOllama } = require('../providers/ollama');\nconst { handleOpenAICompat } = require('../providers/openaiCompat');\n\n// Attach an id to each request (backup; app-level stamp also sets this)\nrouter.use((req, _res, next) => {\n  if (!req.id) req.id = `${Date.now().toString(36)}-${Math.random().toString(36).slice(2,7)}`;\n  next();\n});\n\n// POST /api/chat (and also mounted under /api/llm-chat)\nrouter.post('/chat', async (req, res) => {\n  const rid = req.id;\n  const {\n    provider = 'openai',\n    baseUrl,\n    apiKey,\n    model,\n    messages = [],\n    temperature,\n    max_tokens,\n    stream = true,\n    useResponses = false,\n    reasoning = false,\n    metadata = {}\n  } = req.body || {};\n\n  const conversationId = metadata?.conversationId || req.headers['x-conversation-id'] || null;\n  const clientSource = metadata?.source || 'llm-chat-ui';\n\n  const problems = [];\n  if (!baseUrl) problems.push('baseUrl is required');\n  if (!model) problems.push('model is required');\n  if (!Array.isArray(messages)) problems.push('messages must be an array');\n  if ((provider === 'openai' || provider === 'openai-compatible') && !apiKey) {\n    problems.push('apiKey is required for OpenAI/OpenAI-compatible providers');\n  }\n  if (problems.length) {\n    logWarn('Validation failed', { rid, problems, body: redact(req.body), conversationId, clientSource });\n    return res.status(400).json({ error: 'Validation failed', details: problems });\n  }\n\n  const msgCounts = Array.isArray(messages)\n    ? messages.reduce((acc, m) => { acc[m?.role || 'unknown'] = (acc[m?.role || 'unknown'] || 0) + 1; return acc; }, {})\n    : {};\n\n  const sseMode = !!stream;\n  if (sseMode) {\n    res.setHeader('Content-Type', 'text/event-stream');\n    res.setHeader('Cache-Control', 'no-cache, no-transform');\n    res.setHeader('Connection', 'keep-alive');\n    res.flushHeaders?.();\n  }\n  const sendSSE = (payload) => {\n    if (!sseMode) return;\n    try { res.write(`data: ${JSON.stringify(payload)}\\n\\n`); } catch { try { res.end(); } catch {} }\n  };\n\n  const isGpt5 = /^gpt-5/i.test(model) || /^o5/i.test(model);\n  const autoResponses = isGpt5 && provider === 'openai';\n\n  logInfo('LLM request', {\n    rid, provider, baseUrl, model,\n    stream: !!stream,\n    useResponses: useResponses || autoResponses,\n    reasoning: !!reasoning,\n    temperature: (typeof temperature === 'number' ? temperature : null),\n    max_tokens: max_tokens ?? null,\n    messagesCount: messages.length,\n    roles: msgCounts,\n    conversationId,\n    clientSource\n  });\n  logDebug('LLM messages meta', { rid, messagesCount: messages.length, conversationId });\n\n  // Track SSE connection lifecycle\n  let ended = false;\n  const onEnd = (kind) => {\n    if (ended) return; ended = true;\n    logInfo('LLM request finished', { rid, provider, model, kind, conversationId });\n  };\n  res.on('close', () => onEnd('close'));\n  res.on('finish', () => onEnd('finish'));\n\n  try {\n    if (provider === 'ollama') {\n      return await handleOllama({ res, sendSSE, rid, baseUrl, model, messages, temperature, sseMode, conversationId });\n    }\n    return await handleOpenAICompat({\n      res, sendSSE, rid,\n      baseUrl, apiKey, model, messages, temperature,\n      max_tokens, useResponses: useResponses || autoResponses, reasoning, sseMode,\n      conversationId\n    });\n  } catch (err) {\n    const message = await extractErrAsync(err);\n    logError('LLM fatal error', { rid, message, provider, model, conversationId });\n    if (sseMode) {\n      sendSSE({ type: 'error', message });\n      try { res.end(); } catch {}\n    } else {\n      res.status(500).json({ error: message });\n    }\n  }\n});\n\nmodule.exports = { router };\n"
    },
    {
      "id": "llm-chat-provider-ollama-logging",
      "description": "Include conversationId in provider-level Ollama logs",
      "op": "write_file",
      "path": "modular-framework/modules/llm-chat/server/providers/ollama.js",
      "mode": "overwrite",
      "content": "const axios = require('axios');\nconst { logDebug, logWarn } = require('../logger');\n\nasync function handleOllama({ res, sendSSE, rid, baseUrl, model, messages, temperature, sseMode, conversationId }) {\n  const url = `${baseUrl.replace(/\\/$/, '')}/api/chat`;\n  const body = { model, messages, stream: sseMode };\n  if (typeof temperature === 'number') body.options = { ...(body.options || {}), temperature };\n\n  logDebug('OLLAMA request', { rid, url, body, conversationId });\n\n  if (sseMode) {\n    const response = await axios.post(url, body, { responseType: 'stream' });\n    response.data.on('data', (chunk) => {\n      const lines = chunk.toString().split('\\n').filter(Boolean);\n      for (const line of lines) {\n        try {\n          const evt = JSON.parse(line);\n          if (evt.message && evt.message.content) sendSSE({ type: 'delta', content: evt.message.content });\n          if (evt.done) sendSSE({ type: 'done' });\n        } catch { /* ignore */ }\n      }\n    });\n    response.data.on('end', () => { logDebug('OLLAMA stream end', { rid, conversationId }); res.end(); });\n    response.data.on('error', (e) => { logWarn('OLLAMA stream error', { rid, err: e.message, conversationId }); sendSSE({ type:'error', message: e.message }); res.end(); });\n  } else {\n    const { data } = await axios.post(url, body);\n    const content = data?.message?.content || '';\n    res.json({ content });\n  }\n}\n\nmodule.exports = { handleOllama };\n"
    },
    {
      "id": "llm-chat-provider-openaiCompat-logging",
      "description": "Include conversationId in provider-level OpenAI-compatible logs",
      "op": "write_file",
      "path": "modular-framework/modules/llm-chat/server/providers/openaiCompat.js",
      "mode": "overwrite",
      "content": "const axios = require('axios');\nconst { logDebug, logWarn } = require('../logger');\nconst { extractErrAsync, isUnsupportedParamErrorAsync } = require('../util/http');\n\nfunction handleResponsesChunk(chunk, sendSSE) {\n  const text = chunk.toString();\n  for (const line of text.split('\\n')) {\n    if (!line.startsWith('data:')) continue;\n    const payload = line.replace(/^data:\\s*/, '').trim();\n    if (!payload) continue;\n    try {\n      if (payload === '[DONE]') { sendSSE({ type: 'done' }); continue; }\n      const evt = JSON.parse(payload);\n      const t = evt.type || evt.event || '';\n      if (t === 'response.output_text.delta') {\n        const delta = evt.delta ?? evt.text ?? evt.output_text?.[0]?.content ?? '';\n        if (delta) sendSSE({ type:'delta', content:String(delta) });\n        continue;\n      }\n      if (t === 'response.output_text') {\n        const textOut = evt.output_text?.join?.('') || evt.text || '';\n        if (textOut) sendSSE({ type:'delta', content:String(textOut) });\n        continue;\n      }\n      if (t === 'response.completed') { sendSSE({ type:'done' }); continue; }\n      if (t === 'error' || evt.error) {\n        const message = evt.error?.message || evt.message || 'Unknown error from Responses stream';\n        sendSSE({ type:'error', message }); continue;\n      }\n      const deltaText =\n        evt?.output_text?.[0]?.content ||\n        evt?.delta?.text ||\n        evt?.message?.content ||\n        evt?.content;\n      if (deltaText) sendSSE({ type:'delta', content: deltaText });\n    } catch { /* ignore */ }\n  }\n}\n\nfunction handleChatCompletionsChunk(chunk, sendSSE) {\n  const str = chunk.toString();\n  for (const line of str.split('\\n')) {\n    if (!line.startsWith('data:')) continue;\n    const payload = line.replace(/^data:\\s*/, '').trim();\n    if (!payload) continue;\n    if (payload === '[DONE]') { sendSSE({ type: 'done' }); continue; }\n    try {\n      const json = JSON.parse(payload);\n      const delta = json.choices?.[0]?.delta?.content;\n      if (delta) sendSSE({ type: 'delta', content: delta });\n    } catch { /* ignore */ }\n  }\n}\n\nasync function handleOpenAICompat({\n  res, sendSSE, rid,\n  baseUrl, apiKey, model, messages, temperature,\n  max_tokens, useResponses, reasoning, sseMode,\n  conversationId\n}) {\n  const headers = { 'Content-Type': 'application/json' };\n  if (apiKey) headers['Authorization'] = `Bearer ${apiKey}`;\n\n  const base = baseUrl.replace(/\\/$/, '');\n  const isGpt5 = /^gpt-5/i.test(model) || /^o5/i.test(model);\n\n  if (useResponses) {\n    const url = `${base}/v1/responses`;\n    const rBodyBase = { model, input: messages, stream: sseMode };\n    if (!isGpt5 && typeof temperature === 'number' && !Number.isNaN(temperature)) {\n      rBodyBase.temperature = temperature;\n    }\n    if (!isGpt5 && max_tokens) rBodyBase.max_output_tokens = max_tokens;\n\n    logDebug('RESPONSES request', { rid, url, body: rBodyBase, conversationId });\n\n    try {\n      if (sseMode) {\n        const response = await axios.post(url, rBodyBase, { headers, responseType: 'stream' });\n        response.data.on('data', (chunk) => handleResponsesChunk(chunk, sendSSE));\n        response.data.on('end', () => { logDebug('RESPONSES stream end', { rid, conversationId }); res.end(); });\n        response.data.on('error', (e) => { logWarn('RESPONSES stream error', { rid, err: e.message, conversationId }); sendSSE({ type:'error', message: e.message }); res.end(); });\n      } else {\n        const { data } = await axios.post(url, rBodyBase, { headers });\n        function extractText(x) {\n          if (!x) return '';\n          if (typeof x === 'string') return x;\n          const direct =\n            x.output_text?.join?.('') ||\n            x.message?.content ||\n            x.content;\n          if (direct) return String(direct);\n          const acc = [];\n          const walk = (v) => {\n            if (!v) return;\n            if (typeof v === 'string') { acc.push(v); return; }\n            if (Array.isArray(v)) { v.forEach(walk); return; }\n            if (typeof v === 'object') {\n              if (typeof v.text === 'string') acc.push(v.text);\n              if (typeof v.content === 'string') acc.push(v.content);\n              for (const k of Object.keys(v)) walk(v[k]);\n            }\n          };\n          walk(x);\n          return acc.join('');\n        }\n        const content = extractText(data);\n        logDebug('RESPONSES non-stream result', { rid, empty: !content, topLevelKeys: Object.keys(data || {}), conversationId });\n        if (!content) logWarn('No text extracted from /v1/responses', { rid, conversationId });\n        res.json({ content });\n      }\n    } catch (err) {\n      if (await isUnsupportedParamErrorAsync(err, 'temperature') && rBodyBase.temperature !== undefined) {\n        logWarn('RESPONSES retry without temperature', { rid, conversationId });\n        const rBodyRetry = { ...rBodyBase }; delete rBodyRetry.temperature;\n        if (sseMode) {\n          const response = await axios.post(url, rBodyRetry, { headers, responseType: 'stream' });\n          response.data.on('data', (chunk) => handleResponsesChunk(chunk, sendSSE));\n          response.data.on('end', () => { logDebug('RESPONSES stream end (retry)', { rid, conversationId }); res.end(); });\n          response.data.on('error', (e) => { logWarn('RESPONSES stream error (retry)', { rid, err: e.message, conversationId }); sendSSE({ type:'error', message: e.message }); res.end(); });\n        } else {\n          const { data } = await axios.post(url, rBodyRetry, { headers });\n          const content = data?.output_text?.join?.('') || data?.message?.content || data?.content || '';\n          res.json({ content });\n        }\n        return;\n      }\n      const message = await extractErrAsync(err);\n      sendSSE({ type:'error', message }); try { res.end(); } catch {}\n    }\n    return;\n  }\n\n  const url = `${base}/v1/chat/completions`;\n  const body = { model, messages, stream: sseMode };\n  if (typeof temperature === 'number' && !Number.isNaN(temperature)) body.temperature = temperature;\n  if (max_tokens && !isGpt5) {\n    if (reasoning === true) body.max_completion_tokens = max_tokens;\n    else body.max_tokens = max_tokens;\n  }\n\n  logDebug('CHAT.COMPLETIONS request', { rid, url, body, conversationId });\n\n  if (sseMode) {\n    const response = await axios.post(url, body, { headers, responseType: 'stream' });\n    response.data.on('data', (chunk) => handleChatCompletionsChunk(chunk, sendSSE));\n    response.data.on('end', () => { logDebug('CHAT stream end', { rid, conversationId }); res.end(); });\n    response.data.on('error', (e) => { logWarn('CHAT stream error', { rid, err: e.message, conversationId }); sendSSE({ type:'error', message: e.message }); res.end(); });\n  } else {\n    const { data } = await axios.post(url, body, { headers });\n    const content = data.choices?.[0]?.message?.content || '';\n    res.json({ content });\n  }\n}\n\nmodule.exports = { handleOpenAICompat };\n"
    },
    {
      "id": "llm-chat-splunk-logger-enhancements",
      "description": "Add source/index support to Splunk HEC logger for llm-chat module",
      "op": "write_file",
      "path": "modular-framework/modules/llm-chat/splunk-logger/server/index.js",
      "mode": "overwrite",
      "content": "const os = require('os');\n\nconst SPLUNK_HEC_URL = process.env.SPLUNK_HEC_URL;\nconst SPLUNK_HEC_TOKEN = process.env.SPLUNK_HEC_TOKEN;\nconst SPLUNK_SOURCE = process.env.SPLUNK_SOURCE || 'llm-chat';\nconst SPLUNK_INDEX = process.env.SPLUNK_INDEX || undefined; // optional\n\nconst configured = !!(SPLUNK_HEC_URL && SPLUNK_HEC_TOKEN);\n\nasync function logEvent(level, msg, meta) {\n  if (!configured) return;\n  const payload = {\n    event: {\n      level,\n      message: typeof msg === 'string' ? msg : JSON.stringify(msg),\n      meta\n    },\n    time: Math.floor(Date.now() / 1000),\n    host: os.hostname(),\n    sourcetype: '_json',\n    source: SPLUNK_SOURCE\n  };\n  if (SPLUNK_INDEX) payload.index = SPLUNK_INDEX;\n  try {\n    await fetch(SPLUNK_HEC_URL, {\n      method: 'POST',\n      headers: {\n        'Authorization': `Splunk ${SPLUNK_HEC_TOKEN}`,\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify(payload)\n    });\n  } catch (e) { /* ignore logging failures */ }\n}\n\nfunction logDebug(msg, meta){ return logEvent('debug', msg, meta); }\nfunction logInfo(msg, meta){ return logEvent('info', msg, meta); }\nfunction logWarn(msg, meta){ return logEvent('warn', msg, meta); }\nfunction logError(msg, meta){ return logEvent('error', msg, meta); }\n\nmodule.exports = { logDebug, logInfo, logWarn, logError };\n"
    },
    {
      "id": "compose-enable-splunk-for-llm-chat",
      "description": "Enable Splunk HEC env for llm-chat service and disable console logging",
      "op": "patch_text",
      "path": "modular-framework/docker-compose.yml",
      "patches": [
        {
          "type": "replace_literal",
          "match": "  llm-chat:\n    build: ./modules/llm-chat\n    container_name: llm-chat-module\n    ports:\n      - \"3004:3004\"\n    env_file: ./.env\n    environment:\n      - NODE_ENV=production\n      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}\n      - PORT=3004\n      #- SPLUNK_HEC_URL=$SPLUNK_HEC_URL\n      #- SPLUNK_HEC_TOKEN=$SPLUNK_HEC_TOKEN\n    depends_on:\n      - splunk\n    networks:\n      - app-network\n    restart: unless-stopped\n",
          "replacement": "  llm-chat:\n    build: ./modules/llm-chat\n    container_name: llm-chat-module\n    ports:\n      - \"3004:3004\"\n    env_file: ./.env\n    environment:\n      - NODE_ENV=production\n      - NODE_TLS_REJECT_UNAUTHORIZED=${NODE_TLS_REJECT_UNAUTHORIZED}\n      - PORT=3004\n      - SPLUNK_HEC_URL=${SPLUNK_HEC_URL}\n      - SPLUNK_HEC_TOKEN=${SPLUNK_HEC_TOKEN}\n      - SPLUNK_SOURCE=llm-chat\n      - LOG_TO_CONSOLE=false\n    depends_on:\n      - splunk\n    networks:\n      - app-network\n    restart: unless-stopped\n"
        }
      ]
    }
  ]
}
